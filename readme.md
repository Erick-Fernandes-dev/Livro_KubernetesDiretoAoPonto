

# Kubernetes

---

## Sum√°rio üìì

- [Namespaces](#namespaces)
- [Pods](#Pods)
- [Pods-2](#pods-2)
- [Deployments](#Deployments)
- [Deployments - 2](#Deployments - 2)
- [Services](#Services)
- [Volumes](#Volumes)
- [Cronjobs](#Cronjobs)
- [Secrets](#Secrets)
- [Configmaps](#Configmaps)
- [InitContainer](#InitContainer)
- [RBAC](#RBAC)
- [Helm](#Helm)
- [Ingress](#Ingress)
- [Assuntos-Extra](#Assuntos-Extra)
- [HPA](#HPA)
- [Statefulset](#Statefulset)
- [Extra](#Extra)

---

- **Instalando Clusters**
    
    
    Introdu√ß√£o
    
    - O Kubernetes √© uma plataformas de orquestra√ß√£o de cont√™ineres. A origem dele est√° em um algum lugar nos data centers do Google, onde nasceu o Borg, a plataforma interna de orquestra√ß√£o de cont√™ineres do Google. O Google usou o Borg durante muitos anos para executar suas aplica√ß√µes. Em 2014, a empresa decidiu transferir sua experi√™ncia com o Borg para um novo projeto de c√≥digo aberto chamado de ‚ÄúKubernetes‚Äù (palavra grega que quer dizer ‚ÄúTinomoneiro‚Äù ou ‚ÄúPiloto‚Äù) e, em 2015, passou a ser o primeiro projeto doado para a rec√©m-criada Cloud Native Computing Foundation (CNCF).
    - Comunidade bem ativa, principalmente no github
    - Plataforma como Servi√ßo (Platform-as-a-Service)
    - O Red Hat OpenShift oferece bastante recursos adicionais ao Kubernetes
    
    - Segue uma doc criada por mim para instalar o Kind e kubectl
        
        [https://github.com/Erick-Fernandes-dev/Instalando_o_Kind_e_o_kubectl](https://github.com/Erick-Fernandes-dev/Instalando_o_Kind_e_o_kubectl)
        
    
    **Comando gera instru√ß√µes de conclus√£o autom√°tica espec√≠ficas para o Zsh**
    
    ```bash
    kind completion zsh
    ```
    
    Comando para que o autocomplete do kind funcione permanentemente.
    
    ```bash
    echo "source <(kind completion zsh)" >> ../.zshrc
    ```
    
    ---
    
    ### ***Instalando o Minikube üíª***
    
    ### [Requisitos b√°sicos](https://github.com/badtuxx/DescomplicandoKubernetes/blob/main/pt/day_one/README.md#requisitos-b%C3%A1sicos)
    
    √â importante frisar que o Minikube deve ser instalado localmente, e n√£o em um *cloud provider*. Por isso, as especifica√ß√µes de *hardware* a seguir s√£o referentes √† m√°quina local.
    
    - Processamento: 1 core;
    - Mem√≥ria: 2 GB;
    - HD: 20 GB.
    
    ### **Instala√ß√£o do minikube no linux**
    
    Antes de mais nada, verifique se a sua m√°quina suporta virtualiza√ß√£o. No 
    GNU/Linux, isto pode ser realizado com o seguinte comando:
    
    ```
    grep -E --color 'vmx|svm' /proc/cpuinfo
    
    ```
    
    Caso a sa√≠da do comando n√£o seja vazia, o resultado √© positivo.
    
    H√° a possibilidade de n√£o utilizar um *hypervisor* para a instala√ß√£o do Minikube, executando-o ao inv√©s disso sobre o pr√≥prio host. Iremos utilizar o Oracle VirtualBox como *hypervisor*, que pode ser encontrado [**aqui**](https://www.virtualbox.org/).
    
    Efetue o download e a instala√ß√£o do `Minikube`utilizando os seguintes comandos.
    
    ```
    curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
    
    chmod +x ./minikube
    
    sudo mv ./minikube /usr/local/bin/minikube
    
    minikube version
    ```
    
    Voc√™ pode ent√£o listar os n√≥s que fazem parte do seu *cluster* k8s com o seguinte comando:
    
    ```
    kubectl get nodes
    
    ```
    
    A sa√≠da ser√° similar ao conte√∫do a seguir:
    
    ```
    NAME       STATUS   ROLES           AGE   VERSION
    minikube   Ready    control-plane   20s   v1.25.3
    
    ```
    
    **Para criar um `cluster com mais de um n√≥`, voc√™ pode utilizar o comando abaixo, apenas modificando os valores para o desejado:**
    
    ```
    minikube start --nodes 2 -p multinode-cluster
    
    üòÑ  minikube v1.26.0 on Debian bookworm/sid
    ‚ú®  Automatically selected the docker driver. Other choices: kvm2, virtualbox, ssh, none, qemu2 (experimental)
    üìå  Using Docker driver with root privileges
    üëç  Starting control plane node minikube in cluster minikube
    üöú  Pulling base image ...
    üíæ  Downloading Kubernetes v1.24.1 preload ...
        > preloaded-images-k8s-v18-v1...: 405.83 MiB / 405.83 MiB  100.00% 66.78 Mi
        > gcr.io/k8s-minikube/kicbase: 385.99 MiB / 386.00 MiB  100.00% 23.63 MiB p
        > gcr.io/k8s-minikube/kicbase: 0 B [_________________________] ?% ? p/s 11s
    üî•  Creating docker container (CPUs=2, Memory=8000MB) ...
    üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
        ‚ñ™ Generating certificates and keys ...
        ‚ñ™ Booting up control plane ...
        ‚ñ™ Configuring RBAC rules ...
    üîó  Configuring CNI (Container Networking Interface) ...
    üîé  Verifying Kubernetes components...
        ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
    üåü  Enabled addons: storage-provisioner, default-storageclass
    
    üëç  Starting worker node minikube-m02 in cluster minikube
    üöú  Pulling base image ...
    üî•  Creating docker container (CPUs=2, Memory=8000MB) ...
    üåê  Found network options:
        ‚ñ™ NO_PROXY=192.168.11.11
    üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
        ‚ñ™ env NO_PROXY=192.168.11.11
    üîé  Verifying Kubernetes components...
    üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
    
    ```
    
    Para visualizar os n√≥s do seu novo cluster Kubernetes, digite:
    
    ```
    kubectl get nodes
    
    ```
    
    Inicialmente, a inten√ß√£o do Minikube √© executar o k8s em apenas um n√≥, 
    por√©m a partir da vers√£o 1.10.1 √© poss√≠vel usar a fun√ß√£o de multi-node.
    
    Caso os comandos anteriores tenham sido executados sem erro, a instala√ß√£o do Minikube ter√° sido realizada com sucesso.
    
    ### **Ver detalhes sobre o cluster**
    
    ```
    minikube status
    ```
    
    ---
    
    ### **Descobrindo o endere√ßo do Minikube**
    
    Como dito anteriormente, o Minikube ir√° criar uma m√°quina virtual, assim 
    como o ambiente para a execu√ß√£o do k8s localmente. Ele tamb√©m ir√° 
    configurar o `kubectl` para comunicar-se com o Minikube. Para saber qual √© o endere√ßo IP dessa m√°quina virtual, pode-se executar:
    
    ```
    minikube ip
    
    ```
    
    O endere√ßo apresentado √© que deve ser utilizado para comunica√ß√£o com o k8s.
    
    ---
    
    ### **Acessando a m√°quina do Minikube via SSH**
    
    Para acessar a m√°quina virtual criada pelo Minikube, pode-se executar:
    
    ```
    minikube ssh
    
    ```
    
    ---
    
    ### **Dashboard do Minikube**
    
    O Minikube vem com um *dashboard* *web* interessante para que o usu√°rio iniciante observe como funcionam os *workloads* sobre o k8s. Para habilit√°-lo, o usu√°rio pode digitar:
    
    ```
    minikube dashboard
    
    ```
    
    ---
    
    ### **Logs do Minikube**
    
    Os *logs* do Minikube podem ser acessados atrav√©s do seguinte comando.
    
    ```
    minikube logs
    
    ```
    
    ### **Remover o cluster**
    
    ```
    minikube delete
    
    ```
    
    Caso queira remover o cluster e todos os arquivos referente a ele, utilize o parametro *--purge*, conforme abaixo:
    
    `minikube delete --purge`
    
    ### Instalando e configurando cluster com m√°quinas virtutais com Vagrant
    
    Configura√ß√£o do Vagrantfile
    
    ```bash
    Vagrant.configure("2") do |config|
        (1..1).each do |i|
            config.vm.define "master-#{i}" do |k8s|
                k8s.vm.box = "ubuntu/focal64"
                k8s.vm.hostname = "master-#{i}"
                #k8s.vm.network "private_network", ip: "172.89.0.1#{i}"
                k8s.vm.network "private_network", type: "static", ip: "192.168.56.11#{i}"
    
                k8s.ssh.insert_key = false
                k8s.ssh.private_key_path = ['~/.vagrant.d/insecure_private_key', '~/.ssh/id_rsa']
                k8s.vm.provision "file", source: "~/.ssh/id_rsa.pub", destination: "~/.ssh/authorized_keys"
    
                k8s.vm.provider "virtualbox" do |vb|
                  vb.gui = false
                  vb.cpus = 2
                  vb.memory = "2048"
                end
            end
        end
    
        (1..2).each do |i|
            config.vm.define "worker-#{i}" do |k8s|
                k8s.vm.box = "ubuntu/focal64"
                k8s.vm.hostname = "worker-#{i}"
                #k8s.vm.network "private_network", ip: "172.89.0.2#{i}"
                k8s.vm.network "private_network", type: "static", ip: "192.168.56.12#{i}"
    
                k8s.ssh.insert_key = false
                k8s.ssh.private_key_path = ['~/.vagrant.d/insecure_private_key', '~/.ssh/id_rsa']
                k8s.vm.provision "file", source: "~/.ssh/id_rsa.pub", destination: "~/.ssh/authorized_keys"
    
                k8s.vm.provider "virtualbox" do |vb|
                  vb.gui = false
                  vb.cpus = 1
                  vb.memory = "2048"
                end
            end
        end
    end
    ```
    
    Agora suba as m√°quinas virtuais para o virtualbox
    
    ```bash
    vagrant up
    ```
    
    Abra 3 terminais na pasta onde voc√™ subiu o vagrant e execute o seguinte comando:
    
    ```bash
    vagrant ssh master-1
    vagrant ssh worker-1
    vagrant ssh worker-2
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled.png)
    
    Agora, instale o docker nas 3 m√°quinas
    
    ```bash
    curl -fsSL https://get.docker.com | bash
    ```
    
    Em seguida, nas 3 m√°quinas crie um arquivo daemon.json no diret√≥rio /etc/docker/ e implemente a seguinte configura√ß√£o.
    
    ```bash
    {
        "exec-opts": ["native.cgroupdriver=systemd"],
        "log-driver": "json-file",
        "log-opts": {
    	    "max-size": "100m"
        },
        "storage-driver": "overlay2"
    }
    
    # Salve e saia desse arquivo e execute os seguinte comandos
    #---------------------------------------------------------------
    mkdir -p /etc/systemd/system/docker.service.d
    systemctl daemon-reload
    systemctl restart docker
    docker container ls
    docker info | grep -i cgroup
    ```
    
    Agora instale a chave do pacote google cloud nas 3 m√°quinas
    
    ```bash
    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
    # OK
    ```
    
    Depois instale o kubernetes xenial nas 3 m√°quinas
    
    ```bash
    echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list
    cat /etc/apt/sources.list.d/kubernetes.list
    apt-get update
    
    ```
    
    Agora instale nas 3 m√°quinas: kubeadm, kubelet e kubectl
    
    ```bash
    apt-get install -y kubeadm kubelet kubectl
    ```
    
    Em seguida, execute o seguinte comando na m√°quina master
    
    ```bash
    #Baixar as imagens
    kubeadm config images pull
    ```
    
- **Namespace**
    
    ## Namespaces
    
    - E um lugar isolado, cercadinho da aplicacao
    
    ```
    	# Lista os namespace
    	kubectl get namespace
    
    	# Listar Pods de um determinado mamespace
    	kubecttl get po --namespace kube-flanel
    	kubecttl get po -n kube-flanel
    
    	# Criando um namespace
    	kubectl create namespace <nomeNS>
    	kubectl create ns <nome>
    
    	# Simular a criacao de um namespace com o --dry-run e depois pega o yaml dele
    	kubectl create ns comunidade-devops --dry-run -oyaml
    
    	# Criando um namespace e jogando para dentro de um aqruivo yaml
    	kubectl create ns comunidade-devops --dry-run -oyaml | kubectl neat > ns.yaml
    
    ```
    
    **Yaml de como criar um namespace**
    
    ```yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: comunidade-devops
    
    ```
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
    	name: nginx
    	namespace: comunidade-devops
    spec:
    	containers:
    	- name: mginx
    	  image: nginx:1.14.2
    	  ports:
    	  - containerPort: 80
    
    ```
    
    - O ‚Äî**dry-run** -> Ele finge que criou mas nao criou / simula a criacao de algo
    
    ```
    	#comando para mudar de namespace
    	kubectl config set-context --current --namespace comunidade-devops
    
    ```
    
- **Pod**
    
    
    ## Instalando o kubectl krew e o neat
    
    - kubectl krew √© um tipo de instalador de plugins para o kubenertes
        - Como instalar o krew üëæüêß
        
        [https://krew.sigs.k8s.io/docs/user-guide/setup/install/](https://krew.sigs.k8s.io/docs/user-guide/setup/install/)
        
    - O neat √© um encurtador de yamal. ele retira informa√ß√µes desnecess√°rias de um arquivo de configura√ß√£o de objeto kubernetes yaml
        - **Rode o seguinte comando para instalar o neat**
        
        ```bash
        kubectl krew install neat
        ```
        
    
    ## Pods
    
    - Um pod e como se fosse uma jaula onde os container compartilham do mesmo namespace e do mesmo IP do POD
    
    ```bash
    
    #--image=''--vai baixar uma imagem la do dockerhub
    
    # -it - criar o pod e entrar diretamente nele
    
    # -rm - logo apos de executar o exit, o pod sera removido
    
    kubectl run --image alpine:3.14 --rm -it nomedopod sh
    ```
    
    **Fazer um redirecionamento de configura√ßao yaml terraform para dentro de um arquivo .yaml**
    
    ```bash
    
    kubectl run --image hashicorp/terraform:latest terraform-demo --dry-run -oyaml | kubectl neat > terraform.yaml
    ```
    
    **Executar o SH do terraform**
    
    ```
    kubectl exec -it <nomePod> sh
    ```
    
    ### Init Container
    
    - Init container, sao containers que nao fazem parte do processo principal do pod
    - Sao containers que sao usados apenas na inicializa√ßao
    
    Em Kubernetes, um Init Container (cont√™iner de inicializa√ß√£o) √© um tipo especial de cont√™iner que √© executado antes do cont√™iner principal de um pod. Sua finalidade √© realizar tarefas de inicializa√ß√£o, configura√ß√£o ou prepara√ß√£o necess√°rias antes que o cont√™iner principal seja executado.
    
    Os Init Containers s√£o √∫teis em situa√ß√µes em que √© necess√°rio que certas condi√ß√µes sejam atendidas antes que o aplicativo principal seja iniciado. Por exemplo, um Init Container pode ser usado para realizar as seguintes tarefas:
    
    1. **Configura√ß√£o de volumes:** Um Init Container pode criar ou configurar volumes compartilhados que ser√£o usados pelo cont√™iner principal. Ele pode, por exemplo, montar um volume de configura√ß√£o ou realizar a c√≥pia de arquivos para o volume.
    2. Inicializa√ß√£o de banco de dados: Se o seu aplicativo depende de um banco de dados, um Init Container pode ser usado para aguardar que o banco de dados esteja pronto antes de iniciar o aplicativo principal.
    3. Configura√ß√£o de segredos: Um Init Container pode buscar segredos de um servi√ßo externo ou de um local seguro, como um servi√ßo de gerenciamento de segredos, e fornec√™-los ao cont√™iner principal.
    4. Realiza√ß√£o de verifica√ß√µes de integridade: Um Init Container pode ser usado para executar testes de integridade ou verificar a disponibilidade de servi√ßos externos antes de iniciar o cont√™iner principal.
    5. Configura√ß√£o de vari√°veis de ambiente: O Init Container pode definir vari√°veis de ambiente que ser√£o usadas pelo cont√™iner principal, fornecendo configura√ß√µes ou informa√ß√µes necess√°rias.
    
    Esses s√£o apenas alguns exemplos de casos de uso para Init Containers. Eles fornecem uma maneira flex√≠vel de executar tarefas de inicializa√ß√£o e garantir que as depend√™ncias estejam satisfeitas antes de iniciar o cont√™iner principal de um pod no Kubernetes.
    
    ***Usos comuns:***
    
    - Esperar ate uma URL DNS estar disponivel
    - Codar um repo git
    - A√ßoes que devem ser tomadas antes de inicializar uma aplica√ßao
    
    **Rodando o busybox**
    
    ```bash
    
    kubectl run --image busybox:1.28 --rm -it busybox-demo sh
    ```
    
    ### Nota üìã
    
    O BusyBox √© uma ferramenta de linha de comando que fornece um conjunto de utilit√°rios Unix em um √∫nico execut√°vel compacto. √â projetado para ser executado em sistemas com recursos limitados, como sistemas embarcados, dispositivos IoT ou em ambientes com restri√ß√µes de espa√ßo.
    
    O BusyBox combina v√°rias funcionalidades encontradas em comandos Unix, como ls, cp, mv, cat, grep, entre outros, em um √∫nico bin√°rio. Ele fornece uma maneira conveniente de acessar e executar esses utilit√°rios em um ambiente com recursos restritos.
    
    Al√©m disso, o BusyBox pode ser usado como um cont√™iner leve em ambientes de cont√™ineriza√ß√£o, como o Docker. Ele √© frequentemente usado como um cont√™iner de utilit√°rios em cont√™ineres Kubernetes, onde √© executado como um cont√™iner auxiliar para executar tarefas de manuten√ß√£o, depura√ß√£o ou configura√ß√£o em um pod.
    
    O BusyBox √© flex√≠vel e altamente configur√°vel, permitindo que voc√™ escolha quais utilit√°rios incluir em sua compila√ß√£o. Isso permite que voc√™ ajuste o tamanho e a funcionalidade do BusyBox de acordo com suas necessidades espec√≠ficas.
    
    Em resumo, o BusyBox √© uma ferramenta √∫til para ambientes com recursos limitados, fornecendo uma variedade de utilit√°rios Unix em um √∫nico execut√°vel compacto.
    
    - um binario que vai conter varios binarios
    
    **Executar infinatamente no shell**
    
    ```bash
    
    until nslookup mymysql; do echo "Trying to resolve..."; sleep 1; done
    ```
    
    **Criando e executando um service para o mymysql**
    
    ```
    
    kubectl create service clusterip mymysql --tcp=80:80
    
    ```
    
    **Editando arquivo .yaml para rodar um initContainers, rodando imagem busybox**
    
    ```yaml
    
    apiVersion: v1
    kind: Pod
    metadata:
    	name: nginx-cd
    	namespace: default
    spec:
    	containers:
    	- name: nginx
          image: nginx:1.14.2
          ports:
          - containerPort: 80
    	initContainers:
        - name: waitfordns
        image: busybox:1.28
        command:
    		- sh
    		- -c
    		- until nslookup mymysql; do echo "Trying to resolve..."; sleep 1; done
    
    ```
    
    **Comando para vizualizar logs dentro de um init containers**
    
    ```
    kubectl logs nginx-cd -c waitfordns
    # A flag -c signica qual container queremos ver o log
    
    ```
    
    No contexto do Kubernetes, um multicontainer √© uma abordagem na qual voc√™ empacota e executa v√°rios cont√™ineres relacionados como uma unidade l√≥gica dentro de um pod. Um pod √© a menor unidade de implanta√ß√£o no Kubernetes e pode conter um ou mais cont√™ineres.
    
    Existem v√°rias raz√µes pelas quais voc√™ pode querer usar um multicontainer em Kubernetes. Aqui est√£o alguns casos de uso comuns:
    
    1. **Sidecar containers:** Um sidecar container √© um cont√™iner auxiliar que trabalha em conjunto com o cont√™iner principal dentro do mesmo pod. Ele fornece funcionalidades adicionais, como armazenamento em cache, logging, monitoramento, balanceamento de carga ou qualquer outra tarefa auxiliar necess√°ria pelo cont√™iner principal.
    2. **Containers de logs ou m√©tricas:** Voc√™ pode ter um cont√™iner espec√≠fico dentro do pod dedicado √† coleta e envio de logs ou m√©tricas para uma ferramenta de monitoramento centralizada. Isso ajuda a manter uma separa√ß√£o clara de responsabilidades e facilita o gerenciamento desses componentes.
    3. **Containers de inicializa√ß√£o:** √Äs vezes, voc√™ pode precisar executar um cont√™iner adicional para realizar tarefas espec√≠ficas de inicializa√ß√£o antes que o cont√™iner principal seja iniciado. Isso pode incluir tarefas de pr√©-configura√ß√£o, migra√ß√£o de dados ou outras a√ß√µes necess√°rias antes que o servi√ßo principal esteja pronto para atender solicita√ß√µes.
    
    Ao usar um multicontainer em Kubernetes, √© importante considerar a comunica√ß√£o entre os cont√™ineres. Os cont√™ineres dentro de um pod compartilham o mesmo espa√ßo de rede e podem se comunicar entre si usando a interface de loopback ou acessando servi√ßos expostos por outros cont√™ineres no mesmo pod. Al√©m disso, voc√™ pode usar volumes compartilhados para permitir que os cont√™ineres acessem os mesmos arquivos ou diret√≥rios.
    
    Para definir um multicontainer em Kubernetes, voc√™ precisa criar um arquivo de manifesto (por exemplo, um arquivo YAML) que descreva a configura√ß√£o do pod, incluindo os cont√™ineres, volumes e outras propriedades relevantes. Em seguida, voc√™ pode implantar o pod usando o comando `kubectl apply -f <arquivo_manifesto.yaml>`.
    
    Lembre-se de que o uso de multicontainers pode adicionar complexidade √† sua configura√ß√£o, ent√£o certifique-se de entender claramente os requisitos e as intera√ß√µes entre os cont√™ineres antes de adot√°-los em sua arquitetura Kubernetes.
    
    **Rodando comando dentro de um pod**
    
    ```
    kubectl exec <nomePod> -c <nomeContainer> ls
    
    ```
    
    **Erro: CrashLoopBackOff*** - E um erro de aplicacao, o pod esta tentando subir mas nao consegue, e fica em loop tentando subir
    
    ### troubleshooting: Acessando um container de um pod sem o "kubectl exec"
    
    1. Entrar dentro do SSH Vagrant worker-1
    
    ### Static Pods
    
    - Sao pods que nao sao gerenciados pelo kubelet, sao gerenciados pelo kube-apiserver
    - Sao criados diretamente no worker node
    - Sao criados no diretorio /etc/kubernetes/manifests
    - Sao criados quando o kubelet inicia
    - Sao criados quando o kubelet detecta que um pod nao esta rodando
    
    Os Static Pods s√£o uma forma de criar e gerenciar pods no Kubernetes. Ao contr√°rio dos pods tradicionais, que s√£o gerenciados pelo kubelet atrav√©s do control plane do Kubernetes, os Static Pods s√£o criados e gerenciados diretamente por um kubelet em um n√≥ espec√≠fico.
    
    Quando um kubelet √© configurado para hospedar Static Pods, ele monitora um diret√≥rio local em busca de arquivos de manifesto de pod. Cada arquivo de manifesto representa um Static Pod e cont√©m a defini√ß√£o YAML ou JSON do pod. O kubelet l√™ esses arquivos e cria os pods correspondentes no n√≥ em que est√° sendo executado. Se o kubelet detectar altera√ß√µes nos arquivos de manifesto, ele atualiza os pods em conformidade.
    
    Existem algumas caracter√≠sticas importantes dos Static Pods:
    
    1. **Escopo de n√≥:** Os Static Pods est√£o vinculados a um n√≥ espec√≠fico no cluster Kubernetes. Eles s√£o criados apenas no n√≥ em que o kubelet est√° configurado para hosped√°-los. Essa abordagem √© √∫til quando voc√™ deseja ter pods espec√≠ficos em um n√≥ sem precisar criar um objeto de pod tradicional no control plane do Kubernetes.
    2. Sem reconcilia√ß√£o autom√°tica: Diferentemente dos pods gerenciados pelo control plane do Kubernetes, os Static Pods n√£o s√£o reconciliados automaticamente se falharem ou forem exclu√≠dos. Se um Static Pod falhar, o kubelet n√£o tentar√° reinici√°-lo automaticamente. Voc√™ precisar√° gerenciar manualmente os Static Pods para garantir que eles estejam sempre em execu√ß√£o.
    3. Integra√ß√£o limitada com recursos avan√ßados do Kubernetes: Os Static Pods n√£o t√™m todos os recursos e integra√ß√µes dispon√≠veis para os pods gerenciados pelo control plane do Kubernetes. Eles n√£o s√£o vis√≠veis atrav√©s do API Server e n√£o podem ser escalados, atualizados ou gerenciados por controladores de replica√ß√£o ou conjuntos de replica√ß√£o. Os Static Pods s√£o mais adequados para casos de uso simples e espec√≠ficos em que esses recursos avan√ßados n√£o s√£o necess√°rios.
    
    Os Static Pods podem ser √∫teis em cen√°rios espec√≠ficos, como a execu√ß√£o de servi√ßos de infraestrutura essenciais no n√≥ do Kubernetes, onde voc√™ deseja que esses componentes sejam iniciados e executados antes mesmo do cluster estar totalmente configurado. No entanto, para a maioria dos casos de uso, √© recomend√°vel usar os pods gerenciados pelo control plane do Kubernetes, pois eles oferecem maior flexibilidade e recursos avan√ßados de gerenciamento.
    
    **Comando para deletar todos os Pods**
    
    ```
    kubectl delete pods --all
    
    ```
    
    ### Rodando um pod estaticamente
    
    1. Entrar dentro do SSH Vagrant worker-1 ou worker-2
    2. Entrar no diretorio /etc/kubernetes/manifests
    3. Criar um arquivo chamado pod.yaml
    4. Adicionar o seguinte conteudo:
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      labels:
        run: multicontainer-pod
      name: multicontainer-pod
    spec:
      containers:
      - image: httpd
        name: httpd
      - image: alpine
        name: debug
        command:
          - "sleep"
          - "999999999999"
    
    ```
    
    O pod sera criado automaticamente
    
    **OBS!!!** Quando um Pod e deletado e o arquivo de manifesto nao e deletado, o pod e criado novamente, e preciso deletar o arquivo de manifesto para que o pod nao seja criado novamente
    
    ### lifecycle: Como um pod √© terminado?
    
    Um pod no Kubernetes pode ser terminado de v√°rias maneiras, dependendo da situa√ß√£o e das a√ß√µes tomadas. Aqui est√£o algumas das principais formas pelas quais um pod pode ser terminado:
    
    1. Encerramento volunt√°rio: Um pod pode ser encerrado voluntariamente quando um aplicativo dentro do pod conclui sua tarefa ou √© explicitamente encerrado. Isso pode acontecer quando o processo principal dentro do cont√™iner √© encerrado ou quando um comando de encerramento √© enviado para o cont√™iner. O pod passa do estado "Running" para o estado "Terminated" e √© finalizado.
    2. Erro no cont√™iner: Se um cont√™iner dentro do pod encontrar um erro cr√≠tico ou um estado de falha que n√£o pode ser recuperado, o Kubernetes encerrar√° o pod. Isso pode ocorrer se o cont√™iner principal falhar ao iniciar corretamente, se um health check (verifica√ß√£o de sa√∫de) falhar repetidamente ou se ocorrer uma falha irrepar√°vel no aplicativo dentro do cont√™iner. O pod passa do estado "Running" para o estado "Terminated".
    3. Exclus√£o for√ßada: Um pod pode ser exclu√≠do for√ßadamente pelo usu√°rio ou pelo Kubernetes. Isso pode ser feito usando o comando `kubectl delete pod <nome-do-pod>` ou por meio de a√ß√µes administrativas. Quando um pod √© exclu√≠do for√ßadamente, ele passa do estado "Running" para o estado "Terminating" e, em seguida, para o estado "Terminated". O tempo necess√°rio para a exclus√£o pode variar dependendo de v√°rios fatores, como o tempo de encerramento do cont√™iner e as pol√≠ticas de encerramento configuradas.
    4. Escalonamento autom√°tico: Se o escalonamento autom√°tico estiver habilitado no Kubernetes e os recursos do cluster estiverem esgotados, o Kubernetes pode tomar a decis√£o de encerrar pods existentes para liberar recursos para outros pods em execu√ß√£o. Nesse caso, o pod passa pelo processo normal de encerramento descrito acima.
    
    √â importante notar que, quando um pod √© terminado, ele n√£o √© automaticamente reiniciado, a menos que seja gerenciado por um controlador de replica√ß√£o ou um conjunto de replica√ß√£o que defina uma pol√≠tica de reinicializa√ß√£o. O Kubernetes n√£o reiniciar√° automaticamente pods que foram encerrados, a menos que seja especificamente instru√≠do a faz√™-lo.
    
    ```yaml
    
    apiVersion: v1
    kind: Pod
    metadata:
    	labels:
    		run: worker-pod
    		name: worker-pod
    spec:
    # definindo o tempo de vida do pod
    # tempo de vida do pod / intervalo de tempo para terminar o pod
    terminationGracePeriodSeconds: 60
      containers:
      - image: alpine
        name: alpine
        command:
          - "sleep"
          - "9999999999"
        lifecycle:
          # executa o comando antes de terminar o pod
          preStop:
            exec:
              command:
                - sh
                - -c
                - curl 10.244.2.24
    
    ```
    
    Ver logs do pod httpd
    
    ```
    kubectl logs -f httpd
    
    ```
  
    ## Pods-2
    
    ## **Primeiros passos - parte 1**
    
    - O POD √© a menor unidade do kubernetes
    
    ```bash
    kubeadm token create --print-join-comand
    ```
    
    - Criar um novo token de autentica√ß√£o que permite que um n√≥ se junte a um cluster Kubernetes existente.
    
    ```bash
    
    kubectl get pods
    
    kubectl get pods -n kube-system
    
    kubectl describe pods -n kube-system <pod>
    
    kubectl get pods --all-namespaces -o wide
    
    kubectl get namespaces
    
    # Criando namespace com o nome giropops
    kubectl create namespace giropops
    
    # Criando um pod
    kubectl run nginx --image=nginx
    
    # Pegar o yaml de um objeto
    kubectl get pods nginx -o yaml
    # -o wide -> para ver mais detalhes
    
    # Jogando configura√ß√£o do onbjeto para dentro de um arquivo yaml
    kubectl get pods nginx -o yaml > meu_primeiro_pod.yaml
    
    ```
    
    ### Criando um pod Yaml
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      labels:
        run: nginx
      name: nginx
      namespace: giropops
    spec:
      containers:
      - image: nginx
      # definir a pol√≠tica de pull da imagem do cont√™iner.
        imagePullPolicy: Always
        name: nginx
        # define a pol√≠tica de DNS do cont√™iner.
        dnsPolicy: ClusterFirst
        # define a pol√≠tica de reinicializa√ß√£o do cont√™iner.
        restartPolicy: Always
    ```
    
    **Crie um Alias kubectl no seu .bashrc ou .zshrc**
    
    ```bash
    alias k='kubectl'
    ```
    
    **Vendo o POD rondando no namespace giropops**
    
    ```bash
    kubectl get po -n giropops
    
    kubectl run nginx --image=image --dry-run=client
    
    # O --dry-run=client ele finge que vai criar algo
    
    kubectl run nginx --image=nginx --dry-run=client -o yaml
    kubectl run nginx --image=nginx --dry-run=client -o yaml > meu_segundo_pod.yaml
    ```
    
    ## Primeiros passos - parte 2
    
    ```bash
    kubectl expose pod nginx
    ```
    
    O comando¬†`kubectl expose pod nginx`¬†√© usado para criar um servi√ßo que exp√µe o pod¬†`nginx`¬†na rede. Isso permite que outros pods dentro do cluster acessem o pod¬†`nginx`¬†usando o nome do servi√ßo em vez do endere√ßo IP do pod.
    
    Por padr√£o, o servi√ßo criado por esse comando usa o protocolo TCP e seleciona automaticamente uma porta dispon√≠vel para expor o pod. No entanto, voc√™ pode especificar o protocolo e a porta usando as op√ß√µes¬†`--protocol`¬†e¬†`--port`, respectivamente.
    
    Por exemplo, o comando¬†`kubectl expose pod nginx --port=80 --protocol=TCP`¬†criaria um servi√ßo que exp√µe o pod¬†`nginx`¬†na porta 80 usando o protocolo TCP.
    
    ```bash
    # Pegando informa√ß√£o do meu service
    kubectl describe services nginx
    
    kubectl edit service nginx
    ```
    
    **Escalando deployment:**
    
    ```bash
    kubectl scale --replica=10 deployment nginx
    ```
    
    **Rodando uma porta no pod**
    
    ```bash
    kubectl create deployment --image=nginx nginx --port=80
    ```
    
    **Expondo um deployment** 
    
    ```bash
    kubectl expose deployment niginx
    
    kubectl expose deployment nginx --type=NodePorte
    ```
    
- **Deployments**
    
    ## Deployments
    
    ### O que √© um Deployment?
    
    Um Deployment √© um objeto do Kubernetes que gerencia um conjunto de objetos Pod.
    
    Como o Pod √© a menor unidade de implanta√ß√£o no Kubernetes, o Deployment √© a maneira mais comum de gerenciar Pods.
    
    **Comando para criar um deployment**
    
    ```bash
    
    kubectl create deployment nginx --image=nginx
    
    kubectl create deployment nginx --image=nginx --dry-run
    
    kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
    
    # Criando tr√™s r√©plicas
    kubectl create deployment nginx --image=nginx --replicas=3
    ```
    
    **Mostrar as labels de um determinado Pod**
    
    ```bash
    
    kubectl get pods --show-labels
    ```
    
    **Pegar pod a partir das labels**
    
    ```bash
    
    kubectl get pods -l app=nginx
    ```
    
    **Deletar pods a partir de labels**
    
    ```bash
    
    kubectl delete pods -l app=nginx
    kubectl delete pods -l k8s-app=kube-dns -n kube-system
    ```
    
    ### kubectl explain
    
    **O que e o kubectl explain e para que serve**
    
    O kubectl explain √© um comando que mostra detalhes sobre um determinado objeto
    
    ```bash
    kubectl explain pods
    
    ```
    
    **Mostrar detalhes de um determinado objeto**
    
    ```bash
    kubectl explain pods.spec
    
    ```
    
    ### Grenciando Rollout
    
    **O que e Rollout?**
    
    O termo "Rollout" em Kubernetes refere-se a uma opera√ß√£o de atualiza√ß√£o de implanta√ß√£o de um aplicativo ou servi√ßo em um cluster Kubernetes. O Kubernetes oferece v√°rias estrat√©gias para atualizar aplica√ß√µes de maneira controlada e sem interrup√ß√µes para os usu√°rios finais. Uma dessas estrat√©gias √© o "Rolling Update" (Atualiza√ß√£o Rolante), que √© frequentemente chamado de "Rollout."
    
    Um "Rollout" envolve a atualiza√ß√£o gradual de pods (instancias de cont√™ineres) em uma implanta√ß√£o para garantir que a nova vers√£o do aplicativo seja implantada de forma segura e sem interrup√ß√µes. Durante o processo de Rollout, o Kubernetes gerencia automaticamente a substitui√ß√£o dos pods da vers√£o anterior pelos pods da nova vers√£o. Isso pode ser feito de forma controlada, garantindo que um n√∫mero m√≠nimo de pods esteja sempre em execu√ß√£o e saud√°vel antes de destruir os pods da vers√£o anterior.
    
    O Kubernetes oferece v√°rias maneiras de realizar um Rollout, incluindo estrat√©gias como RollingUpdate, Blue-Green Deployment e Canary Deployment. Cada uma dessas estrat√©gias tem suas pr√≥prias caracter√≠sticas e casos de uso espec√≠ficos.
    
    No contexto do Kubernetes, um Rollout √© uma opera√ß√£o essencial para garantir a disponibilidade e confiabilidade cont√≠nuas de seus aplicativos, permitindo que voc√™ atualize-os de maneira controlada e segura.
    
    **Comando Rollout**
    
    ```bash
    
    # Serve para mostrar o hist√≥rico de um determinado deployment
    
    kubectl rollout history deployment nginx-deployment
    
    kubectl rollout pause deployment nginx-deployment # Pausa o deployment
    
    kubectl rollout resume deployment nginx-deployment # Despausa o deployment
    
    kubectl rollout undo deployment nginx-deployment # Desfaz a ultima altera√ß√£o
    
    kubectl rollout restart deployment nginx-deployment # Reinicia o deployment
    
    kubectl rollout status deployment nginx-deployment # Mostra o status do deployment
    
    kubectl rollout undo deployment/httpd --to-revision=1 # Desfaz o deployment para uma determinada revis√£o
    
    ```
    
    ### maxSurge e maxUnavailable: Otimizando deployment
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: httpd
      name: httpd
    spec:
    # A estrategia de atualizacao do deployment pode ser definida com o campo strategy
      strategy:
        rollingUpdate:
          maxSurge: 10%
          maxUnavailable: 0
      replicas: 100
      selector:
        matchLabels:
          app: httpd
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: httpd
        spec:
          containers:
          - image: httpd
            name: httpd
    
    ```
    
    ```yaml
    
    strategy:
        rollingUpdate:
          maxSurge: 10%
          maxUnavailable: 0
    
    ```
    
    Esse trecho faz parte da defini√ß√£o de estrat√©gia de atualiza√ß√£o de um Deployment no Kubernetes. Nesse caso, a estrat√©gia de atualiza√ß√£o configurada √© uma "RollingUpdate." Vou explicar o que cada parte dessa estrat√©gia faz:
    
    1. `maxSurge`: Isso determina o n√∫mero m√°ximo de pods adicionais que podem ser criados acima do n√∫mero desejado de r√©plicas durante uma atualiza√ß√£o. No seu caso, est√° configurado como "10%". Isso significa que durante uma atualiza√ß√£o, o Kubernetes permitir√° que at√© 10% a mais de pods sejam criados temporariamente, al√©m do n√∫mero desejado de r√©plicas. Isso pode ser √∫til para garantir uma atualiza√ß√£o r√°pida e suave, onde novas r√©plicas s√£o implantadas antes que as antigas sejam removidas.
    2. `maxUnavailable`: Isso determina o n√∫mero m√°ximo de pods que podem ficar indispon√≠veis durante uma atualiza√ß√£o. No seu caso, est√° configurado como "0". Isso significa que o Kubernetes garantir√° que n√£o haja pods indispon√≠veis durante a atualiza√ß√£o. O Kubernetes ir√° garantir que, a qualquer momento, pelo menos o n√∫mero desejado de r√©plicas do aplicativo esteja em execu√ß√£o e funcionando antes de desligar as r√©plicas antigas. Essa configura√ß√£o √© √∫til para garantir alta disponibilidade durante a atualiza√ß√£o.
    
    Portanto, com essa configura√ß√£o espec√≠fica, o Kubernetes est√° garantindo que:
    
    - Durante uma atualiza√ß√£o, at√© 10% a mais de pods podem ser implantados temporariamente (maxSurge) para acelerar a atualiza√ß√£o.
    - N√£o haver√° pods indispon√≠veis (maxUnavailable: 0) durante a atualiza√ß√£o, garantindo alta disponibilidade.
    
    Essas configura√ß√µes podem ser ajustadas de acordo com os requisitos espec√≠ficos de disponibilidade e toler√¢ncia a falhas do seu aplicativo.
    
    ### Escalando um deployment
    
    - Nao e recomendado escalar deployments
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: httpd
      name: httpd
    spec:
    # A estrategia de atualizacao do deployment pode ser definida com o campo strategy
      strategy:
        rollingUpdate:
          maxSurge: 10%
          maxUnavailable: 0
      replicas: 50
      selector:
        matchLabels:
          app: httpd
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: httpd
        spec:
          containers:
          - image: httpd
            name: httpd
    
    ```
    
    **Definir um novo numero de replicas com o Scale**
    
    ```
    	kubectl scale deployment httpd --replicas 110
    
    ```
    
    ### Resources: Como delimitar recursos computacionais?
    
    ![Pasted image 20231019161940.png](Kubernetes%20730e37146baa485d9f46957707434f50/Pasted_image_20231019161940.png)
    
    ![Pasted image 20231019162037.png](Kubernetes%20730e37146baa485d9f46957707434f50/Pasted_image_20231019162037.png)
    
    Em Kubernetes, voc√™ pode limitar os recursos computacionais (CPU e mem√≥ria) que um cont√™iner pode usar em um pod. Isso √© importante para garantir que os recursos do cluster sejam alocados de forma eficiente e que um pod mal comportado n√£o prejudique outros pods no mesmo n√≥. Para definir limites de recursos, voc√™ pode usar os campos `resources.limits` e `resources.requests` no arquivo de especifica√ß√£o do pod. Aqui est√° como voc√™ pode faz√™-lo:
    
    1. **Limites (Limits)**: Os limites s√£o a quantidade m√°xima de recursos que um pod pode usar. Se um pod tentar usar mais recursos do que o limite especificado, ele ser√° limitado a esse valor. Isso √© √∫til para garantir que um pod n√£o sobrecarregue o n√≥ e cause problemas para outros pods. Para definir limites, voc√™ pode adicionar o seguinte ao arquivo de especifica√ß√£o do pod:
        
        ```yaml
        resources:
          limits:
            memory: 512Mi
            cpu: 500m
        
        ```
        
        Neste exemplo, o pod tem um limite de 512 megabytes de mem√≥ria e 500 milicores de CPU (equivalente a 0,5 n√∫cleos).
        
    2. **Requisi√ß√µes (Requests)**: As requisi√ß√µes s√£o a quantidade de recursos que um pod solicita ao cluster. O Kubernetes tentar√° alocar pelo menos essa quantidade de recursos no n√≥ em que o pod ser√° executado. Isso √© √∫til para garantir que o pod tenha recursos m√≠nimos garantidos. Para definir requisi√ß√µes, voc√™ pode adicionar o seguinte ao arquivo de especifica√ß√£o do pod:
        
        ```yaml
        resources:
          requests:
            memory: 256Mi
            cpu: 250m
        
        ```
        
        Neste exemplo, o pod solicita 256 megabytes de mem√≥ria e 250 milicores de CPU.
        
    3. **Exemplo Completo**:
        
        Aqui est√° um exemplo mais completo de como definir limites e requisi√ß√µes de recursos em um arquivo de especifica√ß√£o de pod:
        
        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: meu-pod
        spec:
          containers:
            - name: meu-container
              image: minha-imagem:latest
              resources:
                limits:
                  memory: 512Mi
                  cpu: 500m
                requests:
                  memory: 256Mi
                  cpu: 250m
        
        ```
        
    
    Lembre-se de que essas s√£o apenas diretrizes e os valores exatos de limites e requisi√ß√µes devem ser ajustados de acordo com as necessidades espec√≠ficas de seus aplicativos. Monitorar o desempenho do cluster √© fundamental para garantir que os recursos sejam alocados de maneira eficiente e que os pods estejam recebendo os recursos de que precisam.
    
    ### request & limits na pr√°tica
    
    - Os resources sao especificos de cada container
    
    **Comando para saber mais sobre os resources com o explain**
    
    ```
    kubectl explain deployment.spec.template.spec.containers.resources
    
    ```
    
    **Comando para verificar a quantidade de memoria de um no**
    
    ```
    kubectl top node
    
    ```
    
    **Aplicando o requests e limits**
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: httpd
      name: httpd
    spec:
    # A estrategia de atualizacao do deployment pode ser definida com o campo strategy
      strategy:
        rollingUpdate:
          maxSurge: 10%
          maxUnavailable: 0
      replicas: 1
      selector:
        matchLabels:
          app: httpd
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: httpd
        spec:
          containers:
          - image: httpd
            name: httpd
            resources:
              requests:
                cpu: 100m
                memory: 512M
              limits:
                cpu: 10
                memory: 8192M
    
    ```
    
    ### goldilocks: Como descobrir o request/limits ideal para a aplica√ß√£o
    
    - Goldilocks √© uma ferramenta fant√°stica para definir o request/limits ideal para a aplica√ß√£o.
    
    O "Goldilocks" √© uma ferramenta de c√≥digo aberto criada para o Kubernetes que ajuda a otimizar os recursos de CPU e mem√≥ria alocados para os pods em um cluster Kubernetes. O objetivo principal do Goldilocks √© fornecer recomenda√ß√µes sobre os limites e requisi√ß√µes de recursos apropriados para os pods, com base em m√©tricas de uso de recursos em tempo real.
    
    Aqui est√° para que o Goldilocks Kubernetes √© usado:
    
    1. **Otimiza√ß√£o de Recursos**: O Kubernetes permite que voc√™ defina limites e requisi√ß√µes de recursos para pods, mas determinar os valores ideais pode ser desafiador. Se voc√™ alocar recursos em excesso, pode desperdi√ßar recursos valiosos. Se alocar muito pouco, os pods podem enfrentar problemas de desempenho. O Goldilocks ajuda a encontrar o equil√≠brio certo, otimizando os recursos para maximizar a efici√™ncia.
    2. **Recomenda√ß√µes Din√¢micas**: Em vez de confiar apenas em valores est√°ticos definidos nos arquivos de especifica√ß√£o do pod, o Goldilocks monitora o uso real de CPU e mem√≥ria pelos pods em execu√ß√£o e fornece recomenda√ß√µes din√¢micas com base em m√©tricas observadas.
    3. **Aprimoramento do Desempenho**: Ao garantir que os pods tenham recursos apropriados, o Goldilocks pode ajudar a evitar problemas de desempenho devido a limites muito baixos e reduzir a possibilidade de conten√ß√£o de recursos em um cluster.
    4. **Economia de Recursos**: A otimiza√ß√£o de recursos com o Goldilocks pode economizar dinheiro, especialmente quando voc√™ est√° usando servi√ßos de nuvem que cobram com base nos recursos alocados.
    5. **Integra√ß√£o com Prometheus**: O Goldilocks √© frequentemente usado em conjunto com o Prometheus, um sistema de monitoramento amplamente utilizado no ecossistema Kubernetes. Ele coleta m√©tricas do Prometheus e usa esses dados para gerar recomenda√ß√µes.
    6. **Facilita a Manuten√ß√£o**: Manter as configura√ß√µes de recursos dos pods atualizadas pode ser um desafio, especialmente em clusters de grande escala. O Goldilocks automatiza parte desse processo, tornando a manuten√ß√£o mais eficiente.
    
    Em resumo, o Goldilocks √© uma ferramenta que ajuda a garantir que os recursos em um cluster Kubernetes sejam alocados de maneira eficiente, maximizando o desempenho e minimizando o desperd√≠cio de recursos. Isso √© particularmente √∫til em ambientes de produ√ß√£o onde √© essencial otimizar a utiliza√ß√£o de recursos.
    
    - **[Guia de instala√ß√£o do Goldilocks](https://goldilocks.docs.fairwinds.com/installation/#requirements)**
    
    Outros links interessantes usados na aula:
    
    - **[Download do Helm](https://helm.sh/docs/intro/install/)**
    - **[Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)**
    - **[Values do Helm Chart do Goldilocks](https://artifacthub.io/packages/helm/fairwinds-stable/goldilocks?modal=values)**
    
    **Segue o link para instalar o Goldilocks: [Aqui](https://artifacthub.io/packages/helm/fairwinds-stable/goldilocks)**
    
    - crie um arquivo values.yaml e implemente o seguinte codigo
    
    ```yaml
    
    # uninstallVPA -- Enabling this flag will remove a vpa installation that was previously managed with this chart. It is considered deprecated and will be removed in a later release.
    uninstallVPA: false
    
    vpa:
      # vpa.enabled -- If true, the vpa will be installed as a sub-chart
      enabled: true
      updater:
        enabled: false
    
    metrics-server:
      # metrics-server.enabled -- If true, the metrics-server will be installed as a sub-chart
      enabled: true
      apiService:
        create: true
    
    image:
      # image.repository -- Repository for the goldilocks image
      repository: us-docker.pkg.dev/fairwinds-ops/oss/goldilocks
      # image.tag -- The goldilocks image tag to use
      tag: v4.10.0
      # image.pullPolicy -- imagePullPolicy - Highly recommended to leave this as `Always`
      pullPolicy: Always
    
    # imagePullSecrets -- A list of image pull secret names to use
    imagePullSecrets: []
    
    nameOverride: ""
    fullnameOverride: ""
    
    controller:
      # controller.enabled -- Whether or not to install the controller deployment
      enabled: true
      # controller.revisionHistoryLimit -- Number of old replicasets to retain, default is 10, 0 will garbage-collect old replicasets
      revisionHistoryLimit: 10
      rbac:
        # controller.rbac.create -- If set to true, rbac resources will be created for the controller
        create: true
        # controller.rbac.enableArgoproj -- If set to true, the clusterrole will give access to argoproj.io resources
        enableArgoproj: true
        # controller.rbac.extraRules -- Extra rbac rules for the controller clusterrole
        extraRules: []
        # controller.rbac.extraClusterRoleBindings -- A list of ClusterRoles for which ClusterRoleBindings will be created for the ServiceAccount, if enabled
        extraClusterRoleBindings: []
      serviceAccount:
        # controller.serviceAccount.create -- If true, a service account will be created for the controller. If set to false, you must set `controller.serviceAccount.name`
        create: true
        # controller.serviceAccount.name -- The name of an existing service account to use for the controller. Combined with `controller.serviceAccount.create`
        name:
    
      # controller.flags -- A map of additional flags to pass to the controller
      flags: {}
      # controller.logVerbosity -- Controller log verbosity. Can be set from 1-10 with 10 being extremely verbose
      logVerbosity: "2"
      # controller.nodeSelector -- Node selector for the controller pod
      nodeSelector: {}
      # controller.tolerations -- Tolerations for the controller pod
      tolerations: []
      # controller.affinity -- Affinity for the controller pods
      affinity: {}
      # controller.topologySpreadConstraints -- Topology spread constraints for the controller pods
      topologySpreadConstraints: []
      # controller.resources -- The resources block for the controller pods
      resources:
        limits: {}
        requests:
          cpu: 25m
          memory: 256Mi
      # controller.podSecurityContext -- Defines the podSecurityContext for the controller pod
      podSecurityContext:
        seccompProfile:
          type: RuntimeDefault
      # controller.securityContext -- The container securityContext for the controller container
      securityContext:
        readOnlyRootFilesystem: true
        allowPrivilegeEscalation: false
        runAsNonRoot: true
        runAsUser: 10324
        capabilities:
          drop:
            - ALL
    
      deployment:
        # controller.deployment.extraVolumeMounts -- Extra volume mounts for the controller container
        extraVolumeMounts: []
        # controller.deployment.extraVolumes -- Extra volumes for the controller pod
        extraVolumes: []
        # controller.deployment.annotations -- Extra annotations for the controller deployment
        annotations: {}
        # controller.deployment.additionalLabels -- Extra labels for the controller deployment
        additionalLabels: {}
    
        # controller.deployment.podAnnotations -- Extra annotations for the controller pod
        podAnnotations: {}
    
    dashboard:
      # dashboard.basePath -- Path on which the dashboard is served. Defaults to `/`
      basePath: null
      # dashboard.enabled -- If true, the dashboard component will be installed
      enabled: true
      # dashboard.revisionHistoryLimit -- Number of old replicasets to retain, default is 10, 0 will garbage-collect old replicasets
      revisionHistoryLimit: 10
      # dashboard.replicaCount -- Number of dashboard pods to run
      replicaCount: 2
      service:
        # dashboard.service.type -- The type of the dashboard service
        type: ClusterIP
        # dashboard.service.port -- The port to run the dashboard service on
        port: 80
        # dashboard.service.annotations -- Extra annotations for the dashboard service
        annotations: {}
      # dashboard.flags -- A map of additional flags to pass to the dashboard
      flags: {}
      # dashboard.logVerbosity -- Dashboard log verbosity. Can be set from 1-10 with 10 being extremely verbose
      logVerbosity: "2"
      # dashboard.excludeContainers -- Container names to exclude from displaying in the Goldilocks dashboard
      excludeContainers: "linkerd-proxy,istio-proxy"
      rbac:
        # dashboard.rbac.create -- If set to true, rbac resources will be created for the dashboard
        create: true
        # dashboard.rbac.enableArgoproj -- If set to true, the clusterrole will give access to argoproj.io resources
        enableArgoproj: true
      serviceAccount:
        # dashboard.serviceAccount.create -- If true, a service account will be created for the dashboard. If set to false, you must set `dashboard.serviceAccount.name`
        create: true
        # dashboard.serviceAccount.name -- The name of an existing service account to use for the controller. Combined with `dashboard.serviceAccount.create`
        name:
    
      deployment:
        # dashboard.deployment.annotations -- Extra annotations for the dashboard deployment
        annotations: {}
        # dashboard.deployment.additionalLabels -- Extra labels for the dashboard deployment
        additionalLabels: {}
        # dashboard.deployment.extraVolumeMounts -- Extra volume mounts for the dashboard container
        extraVolumeMounts: []
        # dashboard.deployment.extraVolumes -- Extra volumes for the dashboard pod
        extraVolumes: []
    
        # dashboard.deployment.podAnnotations -- Extra annotations for the dashboard pod
        podAnnotations: {}
    
      ingress:
        # dashboard.ingress.enabled -- Enables an ingress object for the dashboard.
        enabled: false
    
        # dashboard.ingress.ingressClassName -- From Kubernetes 1.18+ this field is supported in case your ingress controller supports it. When set, you do not need to add the ingress class as annotation.
        ingressClassName:
        annotations: {}
          # kubernetes.io/ingress.class: nginx
          # kubernetes.io/tls-acme: "true"
        hosts:
          - host: chart-example.local
            paths:
              - path: /
                type: ImplementationSpecific
    
        tls: []
        #  - secretName: chart-example-tls
        #    hosts:
        #      - chart-example.local
    
      # dashboard.resources -- A resources block for the dashboard.
      resources:
        limits: {}
        requests:
          cpu: 25m
          memory: 256Mi
      # dashboard.podSecurityContext -- Defines the podSecurityContext for the dashboard pod
      podSecurityContext:
        seccompProfile:
          type: RuntimeDefault
      # dashboard.securityContext -- The container securityContext for the dashboard container
      securityContext:
        readOnlyRootFilesystem: true
        allowPrivilegeEscalation: false
        runAsNonRoot: true
        runAsUser: 10324
        capabilities:
          drop:
            - ALL
      nodeSelector: {}
      tolerations: []
      affinity: {}
      # dashboard.topologySpreadConstraints -- Topology spread constraints for the dashboard pods
      topologySpreadConstraints: []
    
    ```
    
    - Coloque true no vpa e metrics-server em enabled
    - Instalando o goldilocks
    
    ```
    helm repo add fairwinds-stable <https://charts.fairwinds.com/stable>
    kubectl create namespace goldilocks
    Helm v2:
    helm install --name goldilocks --namespace goldilocks fairwinds-stable/goldilocks
    Helm v3:
    helm install goldilocks --namespace goldilocks fairwinds-stable/goldilocks
    
    ```
    
    ou
    
    ```
    helm install goldiloks --namespace goldilocks --create-namespace fairwinds-stable/goldilocks -f values.yaml
    
    ```
    
    ![Pasted image 20231020111105.png](Kubernetes%20730e37146baa485d9f46957707434f50/Pasted_image_20231020111105.png)
    
    ```yaml
    kubectl get po -n goldilocks
    
    ```
    
    Vai haver um erro no metricserver, pois estamos utilizando o kind
    
    - Para corrigir vai em:
    
    ```
    # 1
    kubectl get deploy -n goldilocks
    
    #2
    kubectl edit deploy -n goldilocks goldiloks-metrics-server
    
    ```
    
    - Depois edite o arquivo yaml em args e implemente o seguinte:
    
    ```yaml
    - args:
    	- --secure-port=8443
    	- --kubelet-insecure-tls
    
    ```
    
    ```
    kubectl logs -n goldilocks goldilocks-vpa-recomender-867d86856-lp6ct -f
    
    ```
    
    - **port-forward** - vai pegar um porta da maquina atual e vai criar um tunel ate o service dentro do cluster
    
    **Comando para vizualizar labels dos namespaces**
    
    ```
    kubectl get ns --show-labels
    
    ```
    
    **Escolha um namespace de aplicativo e rotule-o assim para ver alguns dados:**
    
    ```
    kubectl label ns goldilocks goldilocks.fairwinds.com/enabled=true
    
    ```
    
    **Vizualizar vpa criado para um determinado namespace:**
    
    ```
    kubectl get vpa -n application
    
    ```
    
    ```
    kubectl create deployment --image nginx nginx -n application --replicas 3
    
    ```
    
    ### Visualizando o Painel
    
    A instala√ß√£o padr√£o cria um servi√ßo ClusterIP para o painel. Voc√™ pode acessar via encaminhamento de porta:
    
    ```
    kubectl -n goldilocks port-forward svc/goldilocks-dashboard 8080:80
    
    ```
    
    Apos de dar um port-forward, digite a url: [http://127.0.0.1:8080/](http://127.0.0.1:8080/)
    
    ![Pasted image 20231020125630.png](Kubernetes%20730e37146baa485d9f46957707434f50/Pasted_image_20231020125630.png)
    
    ![Pasted image 20231020125728.png](Kubernetes%20730e37146baa485d9f46957707434f50/Pasted_image_20231020125728.png)
    
    ```
    kubectl describe vpa -n application goldilocks-http
    
    ```
    
    ### troubleshooting: Como analisar OOM (Out Of Memory) no cluster Kubernetes?
    
    O segredo aqui √© acessar o Node que est√° hosteando o pod e procurar pelo **/var/log/messsages** ou **/var/log/kern.log**.
    
    Dentro do log, voc√™ ter√° uma mensagem parecida com essa aqui:
    
    **Nov¬† 9 16:40:30 k8s-worker-2 kernel: [5189358.480218] Memory cgroup out of memory: Killed process 321757 (java) total-vm:3473912kB, anon-rss:972768kB, file-rss:0kB, shmem-rss:0kB, UID:0 pgtables:2072kB oom_score_adj:918**
    
    √â importante se atentar ao anon-rss ou file-rss, que √© o quanto foi lockado pelo processo da mem√≥ria f√≠sica mesmo. Voc√™ vai tirar alguns insights sobre o consumo e talvez ter uma ideia do que modificar.
    
    Al√©m disso, sempre bom dar uma olhada no sistema de monitoramento por m√©tricas de consumo de RAM para entender mais sobre o que ocorreu.
    
    Estou anexando o kern.log que usei na aula caso voc√™ queira dar uma olhada.
    
    ### Materiais complementares
    
    ![[kern.log]]
    
    **Yaml utilizado para teste:**
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: oom-deployment
      name: oom-deployment
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: oom-deployment
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: oom-deployment
        spec:
          containers:
          - image: valentinomiazzo/jvm-memory-test
            name: oom-deployment
            env:
              - name: "ALLOC_HEAP_MB"
                value: "100"
              - name: "MAX_HEAP_SIZE_MB"
                value: "2000"
            resources:
              requests:
                cpu: 100m
                memory: 512M
              limits:
                cpu: 1
                memory: 1G
    
    ```
    
    O container nada mais do que um processo dentro do sistema operacional
    
    **Comandos para lista contexts e usar contexts (Clusters)**
    
    ```bash
    # listar
    kubectl config get-contexts
    
    # alterar cluster
    kubectl config use-context nome-do-contexto-desejado
    
    ```
    
    Entrar no modo SSH do no
    
    ```bash
    ssh -p 2222 ubuntu@localhost
    
    ```
    
  ## **Deployments - 2**
    
    ### Introdu√ß√£o
    
    - Um Deployment, nada mais √© do que o controller do replicaset
    
    **YAML**
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
    	name: primeiro-deployment
    	labels:
    		# Chave valor
    		run: nginx
    		app: giropops
    		namespace: default
    spec:
    	replicas: 1
    	selector:
    		# tem que se igualar a label do metadata do deployment passado a cima
    		# como se fosse uma heran√ßa
    		matchLabels:
    			run: nginx
    	template:
    		metadata:
    			labels:
    				run: nginx
    				# Entrada, como se tivesse rodando no datacenter do Reino Unido
    				dc: UK
    		spec:
    			containers:
    				- image: nginx
    					imagePullPolicy: Always
    					name: nginx
    					ports:
    					- containerPort: 80
    						protocol: TCP
    					resource: {}
    					terminationMessagePath: /dev/termination-log
             terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
    					
    		
    
    	
    ```
    
    **Explicando cada linha do deployment:** 
    
    - `apiVersion: aps/v1`: Define a vers√£o da API que est√° sendo usada para criar o recurso.
    - `kind: Deployment`: Especifica o tipo de recurso que est√° sendo criado. Neste caso, √© um Deployment.
    - `metadata:`: Fornece dados adicionais sobre o recurso, como nome, namespace, labels e annotations.
        - `labels:`: S√£o pares chave-valor usados para organizar e categorizar recursos.
            - `run: nginx`: Uma label com a chave 'run' e o valor 'nginx'.
            - `app: giropops`: Uma label com a chave 'app' e o valor 'giropops'.
        - `name: primeiro-deployment`: O nome do recurso.
        - `namespace: default`: O namespace onde o recurso ser√° criado.
    - `spec:`: Define o estado desejado para o recurso.
        - `replicas: 3`: Especifica que tr√™s r√©plicas do pod devem ser mantidas em execu√ß√£o.
        - `selector:`: Define como o Deployment encontra quais Pods gerenciar.
            - `matchLabels:`: Uma forma de sele√ß√£o que deve corresponder para que o pod seja selecionado.
                - `app: primeiro-deployment`: A label que deve corresponder.
        - `template:`: Define o template do pod que ser√° usado para criar novos pods.
            - `metadata:`: Metadados para o Pod, incluindo labels e annotations.
                - `labels:`: Labels para o Pod.
                    - `app: primeiro-deployment`: Uma label para o Pod.
            - `spec:`: A especifica√ß√£o do Pod.
                - `containers:`: A lista de cont√™ineres que ser√£o executados no Pod.
                    - `name: primeiro-deployment`: O nome do cont√™iner.
                    - `image: nginx`: A imagem do cont√™iner que ser√° usada.
                    - `imagePullPolicy: Always`: A pol√≠tica de obten√ß√£o de imagem. 'Always' significa que a imagem ser√° sempre puxada.
                    - `ports:`: A lista de portas que ser√£o abertas no cont√™iner.
                        - `containerPort: 80`: A porta que ser√° aberta no cont√™iner.
                    - `protocol: TCP`: O protocolo que ser√° usado para a porta.
                    - `resources: {}`: Os recursos computacionais que ser√£o alocados para o cont√™iner.
                    - `terminationMessagePath: /dev/termination-log`: O caminho para o arquivo que armazenar√° a mensagem de t√©rmino quando o cont√™iner parar.
                    - `terminationMessagePolicy: File`: A pol√≠tica para a mensagem de t√©rmino. 'File' significa que a mensagem ser√° escrita no arquivo especificado acima.
                - `dnsPolicy: ClusterFirst`: A pol√≠tica de DNS para o Pod.
                - `restartPolicy: Always`: A pol√≠tica de rein√≠cio para o Pod. 'Always' significa que o Pod ser√° sempre reiniciado se parar.
                - `schedulerName: default-scheduler`: O nome do agendador que ser√° usado para agendar o Pod.
                - `securityContext: {}`: Define as configura√ß√µes de seguran√ßa para o Pod.
    
    ---
    
    ### Deployment, Label e Node Selector
    
    ```yaml
    kubectl create -f <arquivo.yaml>
    
    kubectl describe deployment <nomeDeployment>
    
    kubectl get replicaset
    
    kubectl describe replicaset <nome>
    
    # Pegar um pod com uma determinada label
    kubectl get pods -l dc=NL
    
    # Pegar um pod com uma determinada label e colocando uma coluna extra
    kubectl get pods -L dc
    
    kubectl get replicaset -l dc=NL
    kubectl get replicaset -L dc=UK
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%201.png)
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%202.png)
    
    **Setando label e um determinado node**
    
    ```bash
    kubectl label nodes <nomeNode> disk=SSD
    kubectl label nodes <nomeNode> disk=HDD
    
    kubectl describe nodes <nomeNode>
    ```
    
    **Listar todos os Labels de um determinado objeto e reescrevendo label**
    
    ```bash
    kubectl label nodes <nomeNode> --list
    kubectl label nodes <nomeNode> app=nginx --overwrite
    ```
    
    **NodeSelector**
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        run: nginx
        app: giropops
      name: terceiro-deployment
      namespace: default
    spec:
      replicas: 3
      # tem que se igualar a label do metadata do deployment passado a cima
      selector:
        matchLabels:
          run: nginx
      template:
        metadata:
          labels:
            run: nginx
            dc: UK
        spec:
          containers:
          - name: terceiro-deployment
            image: nginx
            imagePullPolicy: Always
            ports:
            - containerPort: 80
              protocol: TCP
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          # Vai selecionar o node que tiver o label disk: SSD
          #Senao tiver o label, nao vai subir o pod
          nodeSelector:
            disk: SSD
    ```
    
    ```yaml
    kubectl get pod -o wide
    kubectl describe nodes <nomeNode>
    ```
    
    ---
    
    ### Mais um pouco sobre Labels
    
    **Remover labels:**
    
    ```yaml
    kubectl label nodes <nomeLabel>- --all
    kubectl label node disk- --all
    ```
    
    ---
    
    ### Replicaset
    
    Um ReplicaSet no Kubernetes √© um objeto que garante que um n√∫mero especificado de r√©plicas de um pod estejam sempre em execu√ß√£o. Em outras palavras, um ReplicaSet garante que um pod ou um conjunto homog√™neo de pods estejam sempre dispon√≠veis e em execu√ß√£o.
    
    O ReplicaSet √© frequentemente usado para garantir a disponibilidade de um n√∫mero especificado de Pods id√™nticos. Se os pods estiverem morrendo ou sendo exclu√≠dos, o ReplicaSet garante que novos pods sejam criados para substitu√≠-los e manter o n√∫mero desejado de pods.
    
    Aqui est√° um exemplo de como um ReplicaSet pode ser definido em um arquivo YAML:
    
    ```yaml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: exemplo-replicaset
      labels:
        app: exemplo-app
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: exemplo-app
      template:
        metadata:
          labels:
            app: exemplo-app
        spec:
          containers:
          - name: exemplo-container
            image: exemplo-imagem
    
    ```
    
    Neste exemplo, o ReplicaSet garante que sempre haja tr√™s r√©plicas do pod em execu√ß√£o. Se um pod falhar, o ReplicaSet criar√° um novo pod para substitu√≠-lo.
    
    **Utilizando algumas nomenclatura dos objetos Kubernetes**
    
    ```bash
    #Replicaset
    kubectl get rs 
    #Pod
    kubectl get po
    #Deployment
    kubectl get deploy
    #Service
    kubectl get svc
    #Endpoint
    kubectl get ep
    ```
    
    ```bash
    kubectl describe rs <nomeRS>
    kubectl edit rs <nomeRs>
    ```
    
    OBS! O Replicaset, a fun√ß√£o √© ser um controlador de pods
    
    ---
    
    ### Daemonset
    
    - O DaemonSet √© um objeto que garante que um conjunto de pods esteja em execu√ß√£o em todos os nodes do cluster.
    - Um Daemonset √© praticamente um replicaset com a principal diferen√ßa √© que n√£o escolhe a quantas replicas o pod ir√° possuir.
    - N√£o define a quantidade de r√©plicas.
    - Vai ser utilizado quando √© preciso rodar um pod em todos os n√≥s do cluster.
    - Ex: Imagine. uma ferramenta que est√° coletando dados de todo o cluster, de todos os n√≥s, e quando subir esse cara √© preciso ter a garantia que o mesmo esteja coletando de todos os n√≥s.
    
    **YAML do Deamonset:**
    
    ```yaml
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: nginx-ds
      labels:
        system: strigus
    spec:
      selector:
        matchLabels:
          system: strigus
      template:
        metadata:
          labels:
            system: strigus
        spec:
          containers:
            - name: nginx
              image: nginx:1.7.9
              ports:
                - containerPort: 80
    ```
    
    ```yaml
    kubectl create -f daemonset.yaml
    kubectl get ds
    ```
    
    **Removendo a chave do taint que est√° em NoSchedule**
    
    ```yaml
    kubectl taint node eliot-01 node-role.kubernetes.io/master-
    ```
    
    **Alterando yaml via linha de comando:**
    
    ```yaml
    # Alterando vers√£o da imagem nginx pela linha de comando
    kubectl set image ds nginx-ds nginx=nginx:1.15.0
    ```
    
    **OBS!** S√≥ vai mudar de vers√£o assim que o pod morrer e automaticamente levantar outro.
    
    ---
    
    ### Rollouts e Rollbacks
    
    ```yaml
    kubectl rollout history daemonset nginx-ds
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%203.png)
    
    **Comando para mostrar mais detalhes de uma revis√£o**
    
    ```yaml
    kubectl rollout history daemonset nginx-ds --revision=1
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%204.png)
    
    ```yaml
    kubectl rollout history daemonset nginx-ds --revision=2
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%205.png)
    
    **Fazendo um Rollback**
    
    ```yaml
    kubectl rollout undo daemonset nginx-ds --to-revision=1
    ```
    
    **UpdateStrategy**
    
    ```yaml
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: nginx-ds
      labels:
        system: strigus
    spec:
      selector:
        matchLabels:
          system: strigus
      template:
        metadata:
          labels:
            system: strigus
        spec:
          containers:
            - name: nginx
              image: nginx:1.7.9
              ports:
                - containerPort: 80
    	updateStrategy:
    	  type: RollingUpdate
    ```
    
    - √â um estrat√©gia de atualiza√ß√£o definida como RollingUpdate, o que significa que a atualiza√ß√£o ser√° feita de forma gradual, um node de cada vez, para garantir que o DaemonSet esteja sempre em execu√ß√£o.
    
    ---
    
    ### Canary Deploy
    
    O Canary Deploy √© uma t√©cnica de implanta√ß√£o que permite testar novas vers√µes de um aplicativo em um ambiente de produ√ß√£o com um subconjunto m√≠nimo de usu√°rios antes de fazer um lan√ßamento completo.
    
    No contexto do Kubernetes, o Canary Deploy √© √∫til para:
    
    1. **Reduzir o risco de problemas de produ√ß√£o:** Ao implantar a nova vers√£o para um pequeno grupo de usu√°rios primeiro, voc√™ pode identificar e corrigir problemas antes que eles afetem todos os usu√°rios.
    2. **Obter feedback do usu√°rio:** Voc√™ pode coletar feedback dos usu√°rios do Canary Deploy para fazer melhorias antes do lan√ßamento completo.
    3. **Facilitar revers√µes:** Se algo der errado com a nova vers√£o, voc√™ pode reverter para a vers√£o anterior com interrup√ß√£o m√≠nima para os usu√°rios.
        
        
    
    Exemplo:
    
    1. D√™ um git clone nesse projeto abaixo:
        
        ```bash
        git clone https://github.com/badtuxx/k8s-canary-deploy-example.git
        ```
        
    2. Agora fa√ßa o seguintes passos para implementar:
        
        ```bash
        cp k8s-canary-deploy-example/deploy-app-v1-playbook/roles/common/files/* .
        
        cp k8s-canary-deploy-example/canary-deploy-app-v2-playbook/roles/common/files/* .
        
        cp k8s-canary-deploy-example/deploy-app-v2-playbook/roles/common/files/* .
        
        ```
        
    3. YAML a serem implementado:
        
        Service: service-app.yaml
        
        ```yaml
        apiVersion: v1
        kind: Service
        metadata:
          labels:
            app: giropops
            run: nginx
            track: stable
          name: giropops
          namespace: default
        spec:
          externalTrafficPolicy: Cluster
          ports:
          - nodePort: 32222
            name: http
            port: 80
            protocol: TCP
            targetPort: 80
          - nodePort: 32111
            name: prometheus
            port: 32111
            protocol: TCP
            targetPort: 32111
          selector:
            app: giropops
          sessionAffinity: None
          type: NodePort
        ```
        
        Deployments: app-v1.yaml, app-v2-canary.yaml, app-v2.yaml
        
        ```yaml
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: giropops-v1
        spec:
          replicas: 1
          template:
            metadata:
              labels:
                app: giropops
                version: "1.0.0"
              annotations:
                prometheus.io/scrape: "true"
                prometheus.io/port: "32111"
            spec:
              containers:
              - name: giropops
                image: linuxtips/nginx-prometheus-exporter:1.0.0
                env:
                - name: VERSION
                  value: "1.0.0"
                ports:
                - containerPort: 80
                - containerPort: 32111
        ```
        
        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: giropops-v2
          labels:
            app: giropops
            version: "2.0.0"
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: giropops
              version: "2.0.0"
          template:
            metadata:
              labels:
                app: giropops
                version: "2.0.0"
              annotations:
                prometheus.io/scrape: "true"
                prometheus.io/port: "32111"
            spec:
              containers:
              - name: giropops
                image: linuxtips/nginx-prometheus-exporter:2.0.0
                env:
                - name: VERSION
                  value: "2.0.0"
                ports:
                - containerPort: 80
                - containerPort: 32111
        ```
        
        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: giropops-v2
          labels:
            app: giropops
            version: "2.0.0"
        spec:
          replicas: 10
          selector:
            matchLabels:
              app: giropops
              version: "2.0.0"
          template:
            metadata:
              labels:
                app: giropops
                version: "2.0.0"
              annotations:
                prometheus.io/scrape: "true"
                prometheus.io/port: "32111"
            spec:
              containers:
              - name: giropops
                image: linuxtips/nginx-prometheus-exporter:2.0.0
                env:
                - name: VERSION
                  value: "2.0.0"
                livenessProbe:
                  httpGet:
                    path: /
                    port: 80
                    scheme: HTTP
                readinessProbe:
                  httpGet:
                    path: /
                    port: 80
                    scheme: HTTP
                ports:
                - containerPort: 80
                - containerPort: 32111
        ```
        
        ```yaml
        kubectl create -f app-v1.yaml
        kubectl create -f app-v2-canary.yaml
        kubectl apply -f app-v2.yaml
        ```
        
    4. D√™ um curl e veja a m√°gica acontecer:
        
        ```yaml
        curl 172.18.0.3:32222
        ```
        
        ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%206.png)
        
    
    ---
    
    ### **Um pouco mais de Rollouts e Rollbacks**
    
    **Verficando status de deployment com Rollout**
    
    ```yaml
    kubectl rollout status deloyment <nomeDeployment>
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%207.png)
    
  ## **Services**
    
    ### Introdu√ß√£o
    
    O que √© service?
    
    - √â uma forma de pegar um deployment para que possa ser acessado fora do cluster
    - Uma Camada que possa redirecionar as requisi√ß√µes externas para dentro do deployment.
    
    **Rodar o Autocomplete do kuberntes**
    
    ```bash
    
    echo "source <(kubectl completion bash)" >> /root/.bashrc
    ```
    
    - Certifique-se de que o bash-completion esteja instalado no linux
        
        ```bash
        sudo apt-get install -y bash-completion
        ```
        
    - Agora √© s√≥ utilizar o seguinte comando para que o auto-complete do kubectl funcione
        
        ```bash
        kubectl completion bash > /etc/bash_completion.d/kubectl
        ```
        
    - Agora √© so atualizar
        
        ```bash
        source <(kubectl completion bash)
        ```
        
    
    **OBS: Revisando:**
    
    > **Controllers:** Deployment ‚Üí Replicaset ‚Üí Pod
    > 
    > 
    > **Namespace:** √â como se estivesse seprando por grade o cluster
    > 
    > - kube-system ‚Üí namespace do kubernetes, onde possui os componentes do kubernetes
    > - Default: Onde subimos nossas aplica√ß√µes por padr√£o
    > 
    > **kube-api-server:** √â o cerebro do kubernetes, √© o respons√°vel por efetuar comunica√ß√µes entre os n√≥s
    > 
    > **kube-scheduller:** √â o respons√°vel por determinar em qual n√≥ ser√° hospedado
    > 
    > **kube-controller-manager:** Respons√°vel pelo controle principal
    > 
    > **kube-proxy:** Respons√°vel por  gerenciar a rede dos containers
    > 
    > **CNI:** Respons√°vel por fazer as rede dos PODs funcionarem 
    > 
    
    ---
    
    ### Services
    
    Expor um  Servi√ßo de um determinado POD
    
    ```yaml
    kubectl expose pod nginx --type=ClusterIp
    kubectl expose pod nginx --type=NodePort
    kubectl expose pod nginx --type=LoadBalancer
    ```
    
    **Vizualizar os Services**
    
    ```bash
    kubectl get services
    
    kubectl get svc
    ```
    
    **Vizualizar endpoints**
    
    ```bash
    kubectl get endpoints
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%208.png)
    
    ```bash
    kubectl scale --replicas=10 deployment nginx
    ```
    
    **Deletando o servi√ßo**
    
    ```bash
    kubectl delete service <nome_service>
    ```
    
    **Tipo de Servi√ßo: ClusterIp**
    
    - Comunica√ß√£o interna entre Pods no cluster
    
    **yaml do service**
    
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        run: nginx
      name: meu-nginx
      namespace: default
    spec:
      # Especifica o tipo de clusterIp
      #clusterIP: 10.96.114.95
      ports:
      # Porta do servi√ßo
      - port: 80
        protocol: TCP
        # Porta do pod
        targetPort: 80
      # Vai definir para quem esse servi√ßo ta funcionando
      # Para qual deployment esse servi√ßo esta rodando? Para todo deployment que tiver a label app: nginx
      selector:
        app: nginx
      sessionAffinity: None
      # Tipo de servi√ßo
      type: ClusterIP
    ```
    
    **Criando um service** 
    
    ```yaml
    kubectl create -f <arquivo.yaml>
    ```
    
    O que √© esse **sessionAffinity:**
    
    - Cria um ClientIP para o servi√ßo
    - Vai fazer afinidade pelo endere√ßo do cliente
    - Vai pegar o IP de origem e vai usar o mesmo
    
    **Tipo de Servi√ßo: NodePort**
    
    - Um tipo de servi√ßo em NodePort, √© um servi√ßo que d√° para se comunicar com a aplica√ß√£o externamente do cluster.
    - Quando o NodePort √© criado, ele cria tanto um ClusterIp quanto um NodePort
    
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: nginx
      labels:
        run: nginx
      namespace: default
    spec:
      ports:
        # Especificar uma determinada porta
        # range entre 30000-32767
        - nodePort: 32222
          port: 80
          protocol: TCP
          targetPort: 80
      selector:
        run: nginx
      sessionAffinity: None
      # Tipo de servi√ßo
      type: NodePort
    ```
    
    **Tipo de Servi√ßo: LoadBalancer**
    
    - Toda vez que criar um LoadBalancer, ele cria: ClusterIp, NodePort e um LoadBalancer
    
    **Expondo o tipo de servi√ßo no deployment:**
    
    ```yaml
    kubectl expose deployment nginx --type=LoadBalancer
    
    kubectl get svc
    ```
    
    Ap√≥s criar o servi√ßo do deployment, perceba que  o EXTERNAL-IP est√° em pending: 
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%209.png)
    
    O status "<pending>" para o endere√ßo IP externo (External IP) de um servi√ßo LoadBalancer no Kubernetes geralmente indica que o servi√ßo ainda est√° em processo de provisionamento e aguardando a atribui√ß√£o de um endere√ßo IP externo.
    
    Existem v√°rios motivos para que esse tipo status ocorra:
    
    > **Provisionamento do LoadBalancer:** O provisionamento de 
    um servi√ßo LoadBalancer envolve a intera√ß√£o com o provedor de nuvem 
    subjacente (por exemplo, AWS, GCP, Azure) para alocar um endere√ßo IP 
    externo e configurar as regras do balanceador de carga.
    
    **Problemas de configura√ß√£o:** Erros de configura√ß√£o no 
    recurso de servi√ßo, no controlador de ingresso ou nas permiss√µes do 
    provedor de nuvem podem levar ao atraso na obten√ß√£o do endere√ßo IP 
    externo.
    
    **Cota de recursos esgotada:** Em alguns provedores de nuvem, a atribui√ß√£o de endere√ßos IP externos pode estar sujeita a limites de cota. Se voc√™ atingiu a cota m√°xima de endere√ßos IP externos, pode n√£o ser poss√≠vel provisionar um novo endere√ßo IP at√© que a cota seja aumentada.
    > 
    > 
    > **Problemas no provedor de nuvem**: √Äs vezes, a infraestrutura do provedor de nuvem pode estar enfrentando problemas tempor√°rios que afetam a atribui√ß√£o de endere√ßos IP externos.
    > 
    > **Falta de recursos dispon√≠veis:** Se o provedor de nuvem n√£o tiver recursos de IP externos dispon√≠veis no momento, voc√™ pode ver o status "<pending>" at√© que recursos adicionais estejam dispon√≠veis.
    > 
    > **Cota de recursos esgotada:** Em alguns provedores de nuvem, a atribui√ß√£o de endere√ßos IP externos pode estar sujeita a limites de cota. Se voc√™ atingiu a cota m√°xima de endere√ßos IP externos, pode n√£o ser poss√≠vel provisionar um novo endere√ßo IP at√© que a cota seja aumentada.
    > 
    > **Problemas no provedor de nuvem:** √Äs vezes, a infraestrutura do provedor de nuvem pode estar enfrentando problemas tempor√°rios que afetam a atribui√ß√£o de endere√ßos IP externos.
    > 
    > **Falta de recursos dispon√≠veis:** Se o provedor de nuvem n√£o tiver recursos de IP externos dispon√≠veis no momento, voc√™ pode ver o status "<pending>" at√© que recursos adicionais estejam dispon√≠veis.
    > 
    
    Extraindo o yaml do Service com comando:
    
    ```bash
    kubectl get services nginx -o yaml > meu_primeiro_service_loadbalancer.yaml
    
    # Usando o neat para retirar linhas desnecess√°rias
    
    kubectl get services nginx -o yaml | kubectl neat > meu_primeiro_service_loadbalancer.yaml
    
    ```
    
    Quando um service √© criado, automaticamente um endpoint e gerado
    
    ```bash
    kubectl get endpoints
    
    kubectl describe endpoints
    ```
    
    Aplicando mais de uma configura√ß√£o yaml dentro de um arquivo atrav√©s ‚Äò‚Äî-‚Äô:
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      #annotations:
      name: meu-nginx
      labels:
        run: nginx
      namespace: default
    
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: nginx 
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx
            imagePullPolicy: Always
            ports:
            - containerPort: 80
    
    ---
    
    apiVersion: v1
    kind: Service
    metadata:
      name: nginx
      labels:
        run: nginx
      namespace: default
    spec:
      ports:
        # Especificar uma determinada porta
        # range entre 30000-32767
        - nodePort: 32222
          port: 80
          protocol: TCP
          targetPort: 80
      selector:
        run: nginx
      sessionAffinity: None
      # Tipo de servi√ßo
      type: NodePort
    ```
    
    ---
    
    ### Limitando Recursos
    
    ```yaml
    resources: 
        limits:
    			# Vai garantir no m√°ximo 512Mi
    		   memory: 512Mi
          cpu: "500m"
        requests:
    			# Vai garantir em torno de 256Mi inicial
          memory: 256Mi
          cpu: "250m"
    ```
    
    **limits** ‚Üí √â o total que o kubernetes vai liberar de recursos para  o pod
    
    **requests** ‚Üí √â o quanto √© que ele vai garantir de inicio, ou seja, ele √© respons√°vel por garantir uma determinada quantidade de recursos inicial
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      #annotations:
      name: meu-nginx
      labels:
        run: nginx
      namespace: default
    
    spec:
      replicas: 10
      selector:
        matchLabels:
          app: nginx 
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx
            imagePullPolicy: Always
            ports:
            - containerPort: 80
              protocol: TCP
            resources: 
              limits:
                memory: 512Mi
                cpu: "500m"
              requests:
                memory: 256Mi
                cpu: "250m"
    
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
    ```
    
    ```yaml
    kubectl replace -f deployment.yaml
    ```
    
    - Este √© o comando que indica que voc√™ deseja substituir um recurso existente por um novo recurso definido no arquivo YAML.
    
    **Comando para editar um Service**
    
    ```yaml
    kubectl edit service nginx
    ```
    
    **Executando algo dentro de um container**
    
    ```bash
    kubectl exec -ti meu-pod -- bash
    
    # Dentro do container
    
    apt-get update && apt-get install -y stress
    
    # Teste de stress
    stress --vm 1 --vm-bytes 500M
    ```
    
    ---
    
    ### **LimitRange**
    
    **Crie um namespace**
    
    ```bash
    kubectl create namespace <nome>
    
    # Ver mais detalhes de um namespace
    kubectl describe namespace <nome>
    
    # Extraindo yaml do namespace
    kubectl get namespace <nome> -o yaml | kubectl neat > arquivo.yaml
    ```
    
    ---
    
    **Criar um limitRange**
    
    - √â um objeto usado para limitar a quantidade recurso de determinado namespace
    
    ```yaml
    apiVersion: v1
    kind: LimitRange
    metadata:
      name: limitando-recurso
    spec:
      limits:
      - default:
          cpu: 1
          memory: 256Mi
        defaultRequest:
          cpu: 0.5
          memory: 128Mi
        type: Container
    ```
    
    ---
    
    **Aplicando o limitRange em um determinado namespace**
    
    ```yaml
    kubectl create -f namespace_limitado.yaml -n <namespace>
    
    kubectl get limitranges -n <namespace>
    ```
    
    ![Screenshot from 2023-11-01 22-12-36.png](Kubernetes%20730e37146baa485d9f46957707434f50/Screenshot_from_2023-11-01_22-12-36.png)
    
    ```yaml
    kubectl describe limitranges -n <namespace> <limitrange>
    ```
    
    ---
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2010.png)
    
    **Pod Limitado**
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-limitado
      # Especificar o namespace onde o limitRange foi criado
      namespace: strigus
    spec:
      containers:
        - name: nginx
          image: nginx
    ```
    
    ```yaml
    kubectl create -f pod-limitado.yaml -n <namespace onde foi criado o limitRange>
    
    kubectl describe pods -n <namespace>
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2011.png)
    
    Basta apenas especificar o namespace onde foi criado o limitRanje, que a m√°gica acontece. ü§ì
    
    ---
    
    ### **Taints**
    
    No Kubernetes, os "Taints" s√£o usados para restringir quais n√≥s (Nodes) um pod pode ser agendado. Os Taints s√£o r√≥tulos que s√£o aplicados aos n√≥s, especificando condi√ß√µes sob as quais um pod pode ser agendado neles. Isso √© √∫til para separar recursos em clusters Kubernetes, definindo pol√≠ticas de agendamento e garantindo que determinados pods sejam implantados apenas em n√≥s espec√≠ficos.
    
    Aqui est√° como os Taints funcionam:
    
    1. **Taints em um N√≥:** Um administrador de cluster Kubernetes pode aplicar Taints a um n√≥. Cada Taint consiste em uma chave (key), um valor (value) e um efeito (effect).
    2. **Efeito do Taint:**
        - `NoSchedule`: Isso significa que nenhum pod ser√° agendado no n√≥, a menos que o pod tenha um toler√¢ncia correspondente (Toleration) para esse Taint espec√≠fico. Portanto, que esse cara n√£o √© para schedular mais nenhum container, ou seja, s√≥ vai manter os containers que est√£o em execu√ß√£o dele e n√£o ir√° permitir mais nenhum outro container.
        - `PreferNoSchedule`: Isso indica que o Kubernetes tentar√° evitar agendar pods no n√≥, mas pode faz√™-lo se necess√°rio.
        - `NoExecute`: Isso significa que os pods j√° em execu√ß√£o no n√≥ que n√£o possuem uma toler√¢ncia correspondente ser√£o evitados (ou expulsos).
    3. **Tolera√ß√µes (Tolerations):** Os pods podem especificar toler√¢ncias que correspondem aos Taints aplicados aos n√≥s. Isso permite que um pod seja agendado em n√≥s com Taints espec√≠ficos.
    
    A combina√ß√£o de Taints e Tolerations permite que os administradores de clusters controlem a distribui√ß√£o de pods em seu cluster, garantindo que pods cr√≠ticos ou sens√≠veis a recursos sejam agendados apenas em n√≥s apropriados. Por exemplo, voc√™ pode usar Taints para indicar n√≥s com GPUs ou hardware espec√≠fico e garantir que apenas os pods que podem aproveitar esses recursos sejam agendados nesses n√≥s.
    
    Aqui est√° um exemplo de aplica√ß√£o de um Taint em um n√≥ e uma Toler√¢ncia correspondente em um pod:
    
    1. Aplica√ß√£o de um Taint a um n√≥:
        
        ```
        kubectl taint nodes <node-name> key=value:effect
        ```
        
    2. Especifica√ß√£o de Toler√¢ncia em um pod manifest:
        
        ```yaml
        tolerations:
        - key: "key"
          operator: "Equal"
          value: "value"
          effect: "NoSchedule"
        ```
        
    
    Dessa forma, o pod com a toler√¢ncia correspondente poder√° ser agendado no n√≥ com o Taint espec√≠fico.
    
    Os Taints s√£o √∫teis para casos em que voc√™ precisa garantir que certos n√≥s sejam reservados para cargas de trabalho espec√≠ficas ou para isolar recursos em um cluster Kubernetes.
    
    ```yaml
    kubectl describe nodes <nome_do_node>
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2012.png)
    
    ---
    
    **Fazer com o que node de um determinado node n√£o possa schedular**
    
    ```yaml
    kubecl taint node <nome_do_node> key1=value1:NoSchedule
    
    kubectl describe node <node>
    ```
    
    ---
    
    **Removendo a chave valor do taint**
    
    ```yaml
    kubectl taint node <nome_do_node> key1:NoSchedule-
    ```
    
    ---
    
    **Aplicando um taint NoExecute**
    
    ```yaml
    kubectl taint node <nome_do_node> key1=value1:NoExecute
    ```
    
    Assim que executado, os pods que estavam rodando nesse node migram para outros nodes automaticamente.
    
    ***OBS!*** Nesse caso √© como se estivesse fazendo uma manuten√ß√£o
    
    ---
    
    **Aplicando o taint em todos os nodes**
    
    ```bash
    kubectl taint nodes --all  key1=value1:NoExecute
    ```
    
    Ap√≥s aplicado, nenhum pod pode ser aplicando nesses nodes, ou seja, todo cuidado √© necess√°rio
    
  ## **Volumes**
    
    ### EmptyDir
    
    **EmptyDir** ‚Üí √© um volume que vai ser atribuido um pod. fica grudado no pod e sempre vai iniciar vazio, ou seja, ele √© criado junto do pod
    
    - Todos os containers dentro desse POD, pode fazer qualquer coisa
    - Outro ponto muito importante, √© que n√£o existe persist√™ncia de dados, caso o POD √© removido voc√™ perde seus dados
    
    **Exemplo de um emptyDir em um POD**
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: busybox
      namespace: default
    spec:
      containers:
        - name: busybox
          image: busybox
          command: 
            - sleep
            - "3600"
          # Especificando um volume para o container
          volumeMounts:
            # Aonde o volume ser√° montado dentro do container
            - mountPath: /giropops
            # Nome do volume
              name: giropops-dir
      #Definindo um volume vazio
      volumes:
          # Nome do volume vazio
        - name: giropops-dir
          # Tipo do volume vazio
          emptyDir: {}
    ```
    
    ```yaml
    kubectl describe pod busybox
    
    kubectl exec -ti busybox -- sh
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2013.png)
    
    **Criando um arquivo na pasta que foi criada na configura√ß√£o do POD**
    
    ```yaml
    cd giropops
    touch teste.txt
    ```
    
    **Navegue pelo N√ì de trabalho onde o POD se encontra rodando e localize o arquivo que foi criado dentro do PO**
    
    **OBS!** O exemplo abaixo ser√° feito em um N√ì criado pelo KIND
    
    ```bash
    docker exec -it <nome do no> sh
    
    # V√° em
    cd /var/lib/kubelet/pods
    
    ls
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2014.png)
    
    **Vai aparecer esse valores estranhos, use o comando find para localizar o diret√≥rio**
    
    ```bash
    find . -iname "giropops-dir"
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2015.png)
    
    ---
    
    ### Persistent Volumes (PV)
    
    **PV** ‚Üí Persistent Volume
    
    **PVC** ‚Üí Persistent Volume Clain
    
    - Cria um disco dentro cluster, determinado POD, Deployment‚Ä¶
    - √â preciso que esse pesistent seja conectado h√° algum POD
        - Para isso necess√°rio que tenha um Persistent Volume Clain (PVC)
    
    **Configurando o NFS em uma m√°quina onde se encontra o n√≥ de controle**
    
    ```bash
    
    #Dentro do N√ì
    apt-get update
    
    # Instale isso no N√ì de controle
    apt-get install nfs-kernel-server
    # Instale isso no N√ì de trabalho
    apt-get install  nfs-common
    # Caso algum N√ì n√£o venha instalado p√≥r padr√£o
    apt-get install vim
    
    # Isso no n√≥ de controle
    mkdir /opt/dados
    # Evitar fadiga
    chmod 1777 /opt/dados
    vim /etc/exports
    exportfs -ar
    ```
    
    Dentro do arquivo **exports**
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2016.png)
    
    **Explicando com mais detalhes:**
    
    O arquivo `/etc/exports` √© uma configura√ß√£o para o NFS (Network File System) no Linux. Aqui est√° o que cada parte significa:
    
    - `/opt/dados`: Este √© o diret√≥rio que est√° sendo compartilhado. Os clientes NFS poder√£o acessar os arquivos neste diret√≥rio.
    - ``: Este √© o especificador de host. Neste caso, o asterisco significa que qualquer host pode acessar este compartilhamento.
    - `(rw,sync,no_root_squash,subtree_check)`: Estas s√£o as op√ß√µes de exporta√ß√£o para este compartilhamento.
        - `rw`: Significa que o compartilhamento √© de leitura e grava√ß√£o. Os clientes podem ler e escrever arquivos no compartilhamento.
        - `sync`: Significa que as altera√ß√µes no compartilhamento s√£o escritas imediatamente ao disco, em vez de serem armazenadas em cache primeiro.
        - `no_root_squash`: Por padr√£o, o NFS muda o usu√°rio root nos clientes para um usu√°rio n√£o privilegiado nos servidores, por raz√µes de seguran√ßa. A op√ß√£o `no_root_squash` desativa esse comportamento.
        - `subtree_check`: Esta op√ß√£o habilita a verifica√ß√£o de sub√°rvore, que ajuda a prevenir problemas quando partes de um sistema de arquivos s√£o exportadas. No entanto, pode causar problemas de desempenho em alguns casos, ent√£o √†s vezes √© desativada com `no_subtree_check`.
        
    
    Portanto, esta linha est√° configurando o diret√≥rio `/opt/dados` para ser compartilhado via NFS com qualquer host, permitindo leitura e grava√ß√£o, escrevendo altera√ß√µes imediatamente ao disco, permitindo ao usu√°rio root nos clientes ter privil√©gios de root no servidor, e habilitando a verifica√ß√£o de sub√°rvore.
    
    **O que √© NFS e para que serve?**
    
    NFS, ou Network File System, √© um protocolo de sistema de arquivos distribu√≠dos que permite que os usu√°rios em uma rede de computadores tenham acesso a arquivos e pastas em um servidor de rede. Ele foi desenvolvido pela Sun Microsystems.
    
    Com o NFS, os arquivos e diret√≥rios localizados em um servidor de rede podem ser acessados de maneira semelhante aos arquivos e diret√≥rios locais. Isso √© feito montando o sistema de arquivos remoto no sistema local.
    
    O NFS √© √∫til em ambientes onde os arquivos precisam ser compartilhados entre v√°rios usu√°rios ou sistemas. Por exemplo, em um ambiente de desenvolvimento de software, o c√≥digo-fonte pode ser armazenado em um servidor NFS e acessado por v√°rios desenvolvedores. Ou em um ambiente de escrit√≥rio, os documentos podem ser armazenados em um servidor NFS e acessados por v√°rios funcion√°rios.
    
    No entanto, √© importante notar que o NFS n√£o criptografa o tr√°fego de rede, o que pode ser uma preocupa√ß√£o de seguran√ßa. Para ambientes onde a seguran√ßa √© uma preocupa√ß√£o, outras solu√ß√µes, como o SSHFS (que usa o protocolo seguro SSH), podem ser mais apropriadas.
    
    **YAML do Pesistent Volume (PV)**
    
    Para quem vai utilizar o NFS
    
    ```yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: primeiro-pv
    spec:
      # Tamanho do volume
      capacity:
        storage: 1Gi
      # Acesso ao volume (ReadWriteOnce, ReadOnlyMany, ReadWriteMany)  
      accessModes:
        - ReadWriteMany
      # Pol√≠tica de reten√ß√£o do volume (Retain, Recycle, Delete) 
      # serve para definir o que acontece com o volume quando o pod √© deletado 
      persistentVolumeReclaimPolicy: Retain
      # Definindo o tipo do volume (nfs, hostPath, glusterfs, cinder, cephfs, flocker, iscsi, rbd, flexVolume, vsphereVolume, quobyte, azureFile, photonPersistentDisk, portworxVolume, scaleIO, local, storageos)
      nfs:
        path: /opt/giropops
        server: 172.18.0.4
        readOnly: false
    ```
    
    Para quem vai utilizar um hostPath
    
    ```yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: hostpath-pv
    spec:
      capacity:
        storage: 1Gi
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Retain
      hostPath:
        path: "/opt/app"
    ```
    
    - Crie o seu N√ì o caminho /opt/app
    - Nesete exemplo estou utilizando o kind (Kubernetes in Docker)
    
    ```bash
    docker container exec -it kind-control-plane bash
    
    # Dentro do n√≥ crie essa pasta
    mkdir /opt/app
    
    # Saita do n√≥
    exit
    ```
    
    O campo¬†`accessModes`¬†em um PersistentVolume (PV) no Kubernetes define como o volume pode ser acessado do cluster. Existem tr√™s modos de acesso:
    
    1. `ReadWriteOnce`¬†(RWO): O volume pode ser montado como leitura-grava√ß√£o por um √∫nico n√≥. Este √© o modo mais comum e √© suportado por praticamente todos os tipos de volumes.
    2. `ReadOnlyMany`¬†(ROX): O volume pode ser montado como somente leitura por muitos n√≥s. Isso √© √∫til para compartilhar dados entre pods, mas n√£o todos os tipos de volumes suportam este modo.
    3. `ReadWriteMany`¬†(RWX): O volume pode ser montado como leitura-grava√ß√£o por muitos n√≥s. Isso √© √∫til para compartilhar dados entre pods onde os dados tamb√©m precisam ser alterados, mas nem todos os tipos de volumes suportam este modo.
    
    Nesse exemplo, o PersistentVolume est√° configurado com¬†`ReadWriteMany`, o que significa que o volume pode ser montado como leitura-grava√ß√£o por muitos n√≥s. Isso √© √∫til se voc√™ tiver v√°rios pods que precisam ler e escrever no mesmo volume.
    
    Note que nem todos os tipos de volumes suportam todos os modos de acesso. Por exemplo, volumes do tipo¬†`nfs`¬†suportam todos os tr√™s modos, mas volumes do tipo¬†`gcePersistentDisk`¬†s√≥ suportam¬†`ReadWriteOnce`¬†e¬†`ReadOnlyMany`.
    
    O campo `persistentVolumeReclaimPolicy` em um objeto PersistentVolume (PV) no Kubernetes determina o que acontece com um PV quando ele √© liberado de sua reivindica√ß√£o (PersistentVolumeClaim).
    
    Existem tr√™s pol√≠ticas de recupera√ß√£o poss√≠veis]
    
    1. `Retain`: Esta √© a pol√≠tica que voc√™ tem em seu exemplo. Quando um PV √© liberado, ele n√£o √© automaticamente reciclado ou deletado. Em vez disso, ele permanece com seus dados intactos e passa para o estado `Released`. Isso permite a recupera√ß√£o manual dos dados armazenados no volume. Para reutilizar o volume, √© necess√°rio apagar manualmente o PV e os dados associados.
    2. `Delete`: Quando um PV √© liberado, o Kubernetes ir√° automaticamente deletar o PV e o armazenamento subjacente ser√° tamb√©m deletado.
    3. `Recycle`: Esta pol√≠tica est√° obsoleta a partir do Kubernetes 1.7. Quando um PV √© liberado, os dados no volume s√£o apagados e o volume pode ser reutilizado para uma nova reivindica√ß√£o. No entanto, esta pol√≠tica n√£o √© segura e foi substitu√≠da pela funcionalidade de provisionamento din√¢mico.
    
    Em resumo, a pol√≠tica `Retain` permite que voc√™ mantenha seus dados mesmo depois que o PV √© liberado, mas requer limpeza manual antes que o volume possa ser reutilizado.
    
    **Comando para lista os PVs:**
    
    ```bash
    kubectl get pv
    ```
    
    **Descrever um pv**
    
    ```bash
    kubectl describe pv primeiro-pv
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2017.png)
    
    **YAML de um Persistent Volume Claim (PVC)**
    
    Vai servir para o NFS
    
    ```yaml
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: primeiro-pvc
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        # Tamanho do volume que ser√° requisitado pelo pod
        requests:
          storage: 800Mi
    ```
    
    Vai servir para o hostpath
    
    ```yaml
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: hostpath-pvc
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
      storageClassName: ""
    ```
    
    **Comando para listar os PVCs:**
    
    ```yaml
    kubectl get pvc
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2018.png)
    
    **Montando o volume em um deployment**
    
    ```yaml
    apiVersion: app/v1
    kind: Deployment
    metadata:
      name: hostpath-dp
      labels:
        run: nginx
    spec:
      # tempo de espera para o pod ser considerado morto
      progressDeadlineSeconds: 600
      replicas: 1 
      # Quantidade de revis√µes que ser√£o mantidas
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          run: nginx
      # Estrat√©gia de atualiza√ß√£o do deployment
      strategy:
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 1
        type: RollingUpdate
      template:
        metadata:
          labels:
            run: nginx
        spec:
          containers:
          - image: nginx
            imagePullPolicy: Always
            name: nginx
            # volume que ser√° montado no container
            volumeMounts:
              - name: hostpath-pv
                mountPath: /opt/app
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          # Especificando o volume que ser√° montado no container
          volumes:
            - name: hostpath-pv
              persistentVolumeClaim:
                claimName: hostpath-pvc
          # DNS Policy do pod
          dnsPolicy: ClusterFirst
          # Reiniciar o pod sempre que ele morrer
          restartPolicy: Always
          # scheduler que ser√° utilizado para agendar o pod
          schedulerName: default-scheduler
          # Seguran√ßa do pod
          securityContext: {}
          # maneira de como o pod ser√° finalizado
          terminationGracePeriodSeconds: 30
    ```
    
    ```yaml
    kubectl describe deployment hostpath-dp
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2019.png)
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2020.png)
    
    ---
    
    - **OBS Importante**
        
        Se voc√™ est√© tendo esse problema ao d√° um get no pvc
        
        ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2021.png)
        
        Existem 2 motivos:
        
        1. √© porque o caminho em especifico est√° errado e por isso ele est√° em pendind
        2. J√° o outro √© por causa de que o N√ì de controle n√£o esta configura com o nfs-kernel-server.
        - No meu N√ì em especifico estava tentando d√° um exportfs -ar e me mostrava esse determinado erro **exportfs: /opt/dados does not suport NFS export**, da√≠ eu rodei o seguinte comando:
        
        ```bash
        service nfs-kernel-server status
        
        # ou
        
        systemctl status nfs-kernel-server
        ```
        
        E me veio este erro:
        
        ```bash
        ‚óè nfs-server.service - NFS server and services
             Loaded: loaded (/lib/systemd/system/nfs-server.service; enabled; vendor preset: enabled)
             Active: inactive (dead)
        ```
        
        E em seguida rodei o seguinte comando para d√° um start:
        
        ```bash
        service nfs-kernel-server start
        ```
        
        e me veio outros seguintes Erros
        
        ```bash
        Job for nfs-server.service canceled
        
        root@kind-efk-control-plane:~# service nfs-kernel-server status
        ‚óè nfs-server.service - NFS server and services
             Loaded: loaded (/lib/systemd/system/nfs-server.service; enabled; vendor preset: enabled)
             Active: failed (Result: exit-code) since Wed 2023-11-15 23:31:12 UTC; 5s ago
            Process: 15395 ExecStartPre=/usr/sbin/exportfs -r (code=exited, status=1/FAILURE)
            Process: 15396 ExecStopPost=/usr/sbin/exportfs -au (code=exited, status=0/SUCCESS)
            Process: 15397 ExecStopPost=/usr/sbin/exportfs -f (code=exited, status=0/SUCCESS)
                CPU: 6ms
        
        Nov 15 23:31:12 kind-efk-control-plane systemd[1]: Starting NFS server and services...
        Nov 15 23:31:12 kind-efk-control-plane exportfs[15395]: exportfs: /opt/dados does not support NFS export
        Nov 15 23:31:12 kind-efk-control-plane systemd[1]: nfs-server.service: Control process exited, code=exited, status=1/FAILURE
        Nov 15 23:31:12 kind-efk-control-plane systemd[1]: nfs-server.service: Failed with result 'exit-code'.
        Nov 15 23:31:12 kind-efk-control-plane systemd[1]: Stopped NFS server and services.
        ```
        
        Solu√ß√£o:
        
        - Antes de fazer todos esses procedimentos, eu tinha instalado nesse n√≥ o **nfs-common**, e logo ap√≥s instalei o **nfs-kernel-server**, da√≠ gerou um conflito entre esses dois, e para que o nfs-kernel-server pudesse rodar a solu√ß√£o foi remover o **nfs-common** com o seguinte comando:
            
            ```bash
            apt-get remove nfs-common
            ```
            
        - E em seguida verifiquei o status do nfs-server
            
            ```bash
            root@kind-efk-control-plane:~# systemctl status nfs-kernel-server
            ‚óè nfs-kernel-server.service - LSB: Kernel NFS server support
                 Loaded: loaded (/etc/init.d/nfs-kernel-server; generated)
                 Active: active (exited) since Wed 2023-11-15 23:50:50 UTC; 8s ago
                   Docs: man:systemd-sysv-generator(8)
                Process: 16907 ExecStart=/etc/init.d/nfs-kernel-server start (code=exited, status=0/SUCCESS)
                    CPU: 2ms
            
            Nov 15 23:50:50 kind-efk-control-plane systemd[1]: Starting LSB: Kernel NFS server support...
            Nov 15 23:50:50 kind-efk-control-plane systemd[1]: Started LSB: Kernel NFS server support.
            ```
            
            E estava ativo
            
            Problema solucionado com sucesso‚Ä¶
            
            Fonte da solu√ß√£o:
            
            [https://chat.openai.com/share/b50493c0-2576-4d24-ad97-6d492fa5b811](https://chat.openai.com/share/b50493c0-2576-4d24-ad97-6d492fa5b811)
            
        
  ## **Cronjobs**
    
    
    CronJobs √© um recurso do Kubernetes que permite executar tarefas programadas (jobs) em intervalos espec√≠ficos. √â semelhante √† funcionalidade do cron no Unix/Linux, que executa scripts em hor√°rios programados.
    
    Um CronJob cria Jobs em um cronograma baseado em tempo. O cronograma √© definido usando a sintaxe cron padr√£o.
    
    Aqui est√° um exemplo de um CronJob que executa um Job a cada minuto:
    
    ```yaml
    apiVersion: batch/v1beta1
    kind: CronJob
    metadata:
      name: hello
    spec:
      schedule: "*/1 * * * *"
      jobTemplate:
        spec:
          template:
            spec:
              containers:
              - name: hello
                image: busybox
                args:
                - /bin/sh
                - -c
                - date; echo Hello from the Kubernetes cluster
              restartPolicy: OnFailure
    
    ```
    
    Neste exemplo, o CronJob `hello` est√° programado para rodar a cada minuto. O Job que ele cria executa um √∫nico pod que imprime a data e hora atual e uma mensagem.
    
    Lembre-se de que os CronJobs est√£o em beta no Kubernetes e podem n√£o estar dispon√≠veis em todos os clusters.
    
    outro exemplo:
    
    ```yaml
    apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: giropops-cron
    spec:
      # formato de como escrever um crontabe segue essa sequencia:
      # (minuto, hora, dia, m√™s, dia da semana comando) 
      schedule: "*/1 * * * *"
      jobTemplate:
        spec:
          template:
            spec:
              containers:
              - name: giropops-cron
                image: busybox
                # comando que ser√° executado no container
                args:
                - /bin/sh
                - -c
                - date; echo Sejba bem vindo ao curso de Kubernetes; sleep 30
              restartPolicy: OnFailure
    
    # minutos hora dia mes dia-da-semana comando
    # */1 -> qualquer minuto
    # * -> qualquer dia
    # * -> qualquer m√™s
    # * -> qualquer dia da semana
    # * ->  qualquer comando
    # 10 08 * * 1-5 comando -> executar o comando de segunda a sexta as 08:10
    # 0-59 0-23 1-31 1-12 0-7 
    # */1 1,2 1-10 -> vai executar a cada minuto, nas horas 1 e 2, nos dias 1 a 10
    ```
    
    - `apiVersion: batch/v1`: Define a vers√£o da API do Kubernetes que este objeto pertence. `batch/v1` √© a vers√£o que inclui o tipo de objeto `CronJob`.
    - `kind: CronJob`: Define o tipo de objeto Kubernetes que est√° sendo criado. Neste caso, √© um `CronJob`.
    - `metadata:`: Cont√©m metadados sobre o objeto, como seu nome e namespace.
        - `name: giropops-cron`: Define o nome do CronJob.
    - `spec:`: Define as especifica√ß√µes do CronJob.
        - `schedule: "*/1 * * * *"`: Define o cronograma para o CronJob em formato cron. Neste caso, o job ser√° executado a cada minuto.
        - `jobTemplate:`: Define o template para os Jobs que este CronJob ir√° criar.
            - `spec:`: Define as especifica√ß√µes para os Jobs.
                - `template:`: Define o template para os Pods que o Job ir√° criar.
                    - `spec:`: Define as especifica√ß√µes para os Pods.
                        - `containers:`: Define a lista de cont√™ineres que o Pod ir√° executar.
                            - `name: giropops-cron`: Define o nome do cont√™iner.
                            - `image: busybox`: Define a imagem Docker que o cont√™iner ir√° usar.
                            - `args:`: Define os argumentos que ser√£o passados para o cont√™iner no momento da inicializa√ß√£o.
                                - `/bin/sh`
                                - `-c`
                                - `date; echo Sejba bem vindo ao curso de Kubernetes; sleep 30`: Estes comandos ser√£o executados no cont√™iner. Primeiro, ele imprimir√° a data e hora atual, depois imprimir√° a mensagem "Sejba bem vindo ao curso de Kubernetes", e ent√£o dormir√° por 30 segundos.
                        - `restartPolicy: OnFailure`: Define a pol√≠tica de reinicializa√ß√£o para os cont√™ineres no Pod. Neste caso, se um cont√™iner falhar, ele ser√° reiniciado.
    
    ```yaml
    kubectl get cronjobs
    
    kubectl get jobs
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2022.png)
    
    Verfique as infroma√ß√µes desse cronjob e depois verifique os logs
    
    ```bash
    kubectl describe cronjobs giropops-cron
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2023.png)
    
    ```bash
    kubectl logs giropops-cron-28343206-npxm2
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2024.png)
    
  ## **Secrets**
    
    
    Secrets no Kubernetes s√£o um objeto que cont√©m um pequeno volume de dados sens√≠veis, como uma senha, um token ou uma chave. Essas informa√ß√µes podem ser usadas por pods para realizar a√ß√µes.
    
    Os Secrets s√£o √∫teis para armazenar informa√ß√µes que voc√™ n√£o quer que sejam expostas em seu aplicativo, como credenciais de acesso a um banco de dados. Eles s√£o armazenados no cluster Kubernetes e podem ser montados em um pod como um volume ou expostos como vari√°veis de ambiente.
    
    - **secret**  - Sempre quando colocar uma chave valor, e esses chave valor n√£o ficar dispon√≠vel diretamente no c√≥digo.
    
    Aqui est√° um exemplo de como criar um Secret com um par de nome de usu√°rio e senha:
    
    ```yaml
    apiVersion: v1
    kind: Secret
    metadata:
      name: mysecret
    type: Opaque
    data:
      username: YWRtaW4=  # Base64 encoded 'admin'
      password: MWYyZDFlMmU2N2Rm  # Base64 encoded '1f2d1e2e67df'
    
    ```
    
    Neste exemplo, o Secret chamado `mysecret` cont√©m dois dados: `username` e `password`. Os valores s√£o codificados em Base64.
    
    Lembre-se de que os Secrets n√£o s√£o uma forma segura de armazenar informa√ß√µes sens√≠veis, pois o conte√∫do dos Secrets pode ser obtido por qualquer usu√°rio com acesso ao cluster. Eles s√£o apenas uma maneira de evitar que essas informa√ß√µes sejam expostas em seu c√≥digo. Para uma seguran√ßa mais robusta, voc√™ deve considerar solu√ß√µes adicionais, como o uso de uma ferramenta de gerenciamento de segredos externa.
    
    **Criando um Secret baseado no arquivo secret.txt**
    
    ```bash
    echo -n "Diodai Dodinho Yodai" > secret.txt
    
    kubectl create secret generic my-secret --from-file=secret.txt
    ```
    
    Explicando a linha de comando pasada:
    
    Este comando cria um objeto Secret no Kubernetes usando o comando `kubectl create secret`. Aqui est√° o que cada parte do comando faz:
    
    - `kubectl`: Esta √© a ferramenta de linha de comando que permite interagir com o cluster Kubernetes.
    - `create secret`: Este comando cria um novo objeto Secret.
    - `generic`: Este √© o tipo de Secret que est√° sendo criado. Um Secret gen√©rico √© um Secret que pode conter qualquer dado, mas n√£o tem um esquema espec√≠fico.
    - `my-secret`: Este √© o nome que voc√™ est√° dando ao Secret.
    - `-from-file=secret.txt`: Esta op√ß√£o indica que o Secret deve ser preenchido com o conte√∫do do arquivo `secret.txt`. Cada linha no arquivo ser√° transformada em um item no Secret.
    
    Ent√£o, se o conte√∫do do arquivo `secret.txt` √© `Diodai Dodinho Yodai`, o Secret criado ter√° um item com a chave `secret.txt` e o valor `Diodai Dodinho Yodai`.
    
    Lembre-se de que os valores em um Secret s√£o codificados em base64, ent√£o o valor real armazenado ser√° a string `Diodai Dodinho Yodai` codificada em base64.
    
    ```bash
    kubectl describe secrets my-secret
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2025.png)
    
    ```bash
    # Pegar todos os secrets criados
    kubectl get secrets
    
    #Pegar o yaml de um determinado secret criado
    kubectl get secrets my-secret -oyaml
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2026.png)
    
    - S√£o codificados em base64
    - Para decodificar, pegue o valor do token criado pelo secret, no exemplo acima pegue este **RGlvZGFpIERvZGluaG8gWW9kYWk=** execute o seguinte comando:
    
    ```bash
    echo '**RGlvZGFpIERvZGluaG8gWW9kYWk=**' | base64 --decode
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2027.png)
    
    **Criando Secret de forma literal**
    
    ```bash
    kubectl create secret generic my-literal-secret --from-literal user=wolf --from-literal password=tbwa0002
    ```
    
    **Describe**
    
    ```bash
    kubectl describe secret my-literal-secret
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2028.png)
    
    **Criando um pod e inserindo vari√°veis de ambiente com os valores do secret**
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: teste-secret-env
      namespace: default
    spec:
      containers:
        - image: busybox
          name: busy-secret-env
          command:
            - sleep
            - "3600"
          env:
          - name: MEU_USERNAME
            #Valor de algum lugar, ou seja, de uma secretkeyref
            valueFrom:
              # Valor da chave user do secret my-literal-secret que ser√° usado na vari√°vel MEU_USERNAME
              secretKeyRef:
                name: my-literal-secret
                # Chave do secret my-literal-secret que ser√° usado na vari√°vel MEU_USERNAME
                key: user
          - name: MEU_PASSWORD
            valueFrom:
              secretKeyRef:
                name: my-literal-secret
                key: password
    ```
    
    ```yaml
    kubectl get secret
    
    kubectl exec -it teste-secret-env -- sh
    
    # Dentro do pod digite o seguinte comando para vizualizar as vari√°veis de ambiente
    set
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2029.png)
    
  ## **Configmaps**
    
    
    √â um recurso que permite armazenar configura√ß√µes n√£o confidenciais em formato de pares chave-valor. Essas configura√ß√µes podem ser utilizadas pelos pods durante a execu√ß√£o. O objetivo principal do ConfigMap √© separar a configura√ß√£o do c√≥digo-fonte da aplica√ß√£o, permitindo que voc√™ altere a configura√ß√£o sem modificar o c√≥digo da aplica√ß√£o.
    
    Aqui est√£o alguns pontos-chave sobre ConfigMaps no Kubernetes:
    
    1. **Formato de Dados:**
        - O ConfigMap armazena dados como pares chave-valor.
        - Esses dados podem incluir configura√ß√µes de ambiente, configura√ß√µes de inicializa√ß√£o ou qualquer outra informa√ß√£o n√£o sens√≠vel.
    2. **Cen√°rios de Uso:**
        - Configura√ß√µes de ambiente para containers/pods.
        - Configura√ß√µes para aplicativos.
        - Configura√ß√µes para volumes.
    3. **Cria√ß√£o de ConfigMap:**
        - Voc√™ pode criar um ConfigMap diretamente usando arquivos de defini√ß√£o YAML ou atrav√©s do comando `kubectl create configmap`.
        - Exemplo de cria√ß√£o usando YAML:
            
            ```yaml
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: meu-configmap
            data:
              CHAVE1: valor1
              CHAVE2: valor2
            ```
            
    4. **Referenciando ConfigMaps em Pods:**
    - Os dados do ConfigMap podem ser injetados nos pods como vari√°veis de ambiente ou volumes.
    - Exemplo de refer√™ncia em um pod usando vari√°veis de ambiente:
        
        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: meu-pod
        spec:
          containers:
          - name: meu-container
            image: minha-imagem
            env:
            - name: CHAVE1
              valueFrom:
                configMapKeyRef:
                  name: meu-configmap
                  key: CHAVE1
            - name: CHAVE2
              valueFrom:
                configMapKeyRef:
                  name: meu-configmap
                  key: CHAVE2
        ```
        
    1. **Atualiza√ß√£o Din√¢mica:**
        - Altera√ß√µes nos dados do ConfigMap podem ser refletidas automaticamente nos pods que referenciam esses dados.
        - Isso facilita a atualiza√ß√£o din√¢mica da configura√ß√£o sem reiniciar os pods.
    2. **Volume Baseado em ConfigMap:**
        - Al√©m de vari√°veis de ambiente, voc√™ pode montar um volume baseado em ConfigMap diretamente nos pods.
        - Isso permite que arquivos de configura√ß√£o sejam acessados pelo caminho do sistema de arquivos do cont√™iner.
        
    
    Rodando um configmap via linha de comando:
    
    ```yaml
    kubectl create configmap cores-frutas --from-literal uva=roxa --from-file=predileta --from-file=frutas/
    
    kubectl get configmap
    
    kubectl describe configmap cores-frutas
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2030.png)
    
    YAML do configmap de cores-frutas
    
    ```yaml
    apiVersion: v1
    data:
      banana: |
        amarela
      limao: |
        verde
      melancia: |
        Verde e vermelho
      morango: |
        vermelho
      predileta: |
        kiwi
      uva: roxa
    kind: ConfigMap
    metadata:
      name: cores-frutas
      namespace: default
    ```
    
    - `apiVersion: v1`: Define a vers√£o da API do Kubernetes que este objeto pertence.
    - `kind: ConfigMap`: Define o tipo de objeto Kubernetes que est√° sendo criado. Neste caso, √© um¬†`ConfigMap`.
    - `metadata:`: Cont√©m metadados sobre o objeto, como seu nome e namespace.
        - `name: cores-frutas`: Define o nome do ConfigMap.
        - `namespace: default`: Define o namespace onde o ConfigMap ser√° criado. Se n√£o especificado, o namespace¬†`default`¬†√© usado.
    - `data:`: Cont√©m os dados que ser√£o armazenados no ConfigMap. Cada chave sob¬†`data`¬†se torna o nome de um arquivo, e o valor da chave se torna o conte√∫do do arquivo quando o ConfigMap √© montado em um Pod.
        - `banana: | amarela`,¬†`limao: | verde`,¬†`melancia: | Verde e vermelho`,¬†`morango: | vermelho`,¬†`predileta: | kiwi`,¬†`uva: roxa`: Estes s√£o os pares chave-valor que ser√£o armazenados no ConfigMap. A barra vertical¬†`|`¬†permite que voc√™ insira um valor de v√°rias linhas. Neste caso, cada fruta tem uma cor associada.
    
    Referenciando um configmap em um POD:
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: busybox-configmap
      namespace: default
    spec:
      containers:
        - image: busybox
          name: busy-configmap
          command:
           - sleep
           - "3600"
          env:
          - name: frutas
            valueFrom:
              configMapKeyRef:
                name: cores-frutas
                key: predileta
    ```
    
    ```yaml
    kubectl exec -ti busybox-configmap -- sh
    
    # Dentro do pod
    
    set
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2031.png)
    
    ```yaml
    kubectl delete -f pod-configmap.yaml
    ```
    
    Agora suba um pod que referencia todas as chaves de cores de frutas
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: busybox-configmap
      namespace: default
    spec:
      containers:
        - image: busybox
          name: busy-configmap
          command:
           - sleep
           - "3600"
          envFrom:
          # traz todas as chaves do configmap cores-frutas
          - configMapRef:
              name: cores-frutas
    ```
    
    ```yaml
    kubectl exec -ti busybox-configmap -- sh
    
    # Dentro do pod
    
    set
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2032.png)
    
    Criando um pod com configmap dentro de um arquivo
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: busybox-configmap-file
      namespace: default
    spec:
      containers:
        - image: busybox
          name: busy-configmap
          command:
            - sleep
            - "3600"
          volumeMounts:
          - name: meu-configmap-vol
            mountPath: /etc/frutas
      volumes:
      - name: meu-configmap-vol
        configMap:
          name: cores-frutas
    ```
    
    ```yaml
    kubectl exec -ti busybox-configmap-file -- sh
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2033.png)
    
  ## **InitContainer**
    
    
    Executa uma tarefa antes do meu POD principal
    
    - Imagine uma aplica√ß√£o, e para a aplica√ß√£o rode, antes √© preciso que rodar um comando para gerar dados, logo, vai exisitir um outro container que vai executar essa determinada tarefa antes da aplica√ß√£o
    - Um initContainer ‚Üí √â um container que vai executar uma a√ß√£o espec√≠fica antes do container propriamente dito ser executado
    
    YAML de um pod initContainer
    
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: init-demo
      labels:
        app: init-demo
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
          volumeMounts:
            - name: workdir
              mountPath: /usr/share/nginx/html
      initContainers:
        - name: install
          image: busybox
          # vai fazer o download do index.html do site kubernetes.io
          command:
          - wget
          - "--no-check-certificate"
          - "-O"
          - "/work-dir/index.html"
          - https://www.docker.com/
          volumeMounts:
            - name: workdir
              mountPath: "/work-dir"
      dnsPolicy: Default
      volumes:
        - name: workdir
          emptyDir: {}
    ```
    
    - `apiVersion: v1`: Define a vers√£o da API do Kubernetes que este objeto pertence.
    - `kind: Pod`: Define o tipo de objeto Kubernetes que est√° sendo criado. Neste caso, √© um¬†`Pod`.
    - `metadata:`: Cont√©m metadados sobre o objeto, como seu nome e labels.
        - `name: init-demo`: Define o nome do Pod.
        - `labels:`: Define os labels do Pod, que s√£o usados para identificar o Pod entre outros objetos no Kubernetes.
    - `spec:`: Define as especifica√ß√µes do Pod.
        - `containers:`: Define a lista de cont√™ineres que o Pod ir√° executar.
            - `name: nginx`: Define o nome do cont√™iner.
            - `image: nginx`: Define a imagem Docker que o cont√™iner ir√° usar.
            - `ports:`: Define as portas que o cont√™iner ir√° expor.
            - `volumeMounts:`: Define os volumes que ser√£o montados no cont√™iner.
        - `initContainers:`: Define a lista de cont√™ineres de inicializa√ß√£o que ser√£o executados antes dos cont√™ineres principais.
            - `name: install`: Define o nome do cont√™iner de inicializa√ß√£o.
            - `image: busybox`: Define a imagem Docker que o cont√™iner de inicializa√ß√£o ir√° usar.
            - `command:`: Define o comando que ser√° executado quando o cont√™iner de inicializa√ß√£o iniciar. Neste caso, ele est√° fazendo o download da p√°gina inicial do site Docker e salvando-a como¬†`index.html`¬†no diret√≥rio de trabalho.
        - `dnsPolicy: Default`: Define a pol√≠tica de DNS para o Pod.
        - `volumes:`: Define os volumes que ser√£o disponibilizados para os cont√™ineres no Pod.
        
        Porqu√™ ele baixou o o site pelo busybox e a mesma est√° funcianando no container do nginx?
        
        O cont√™iner `install` √© um cont√™iner de inicializa√ß√£o (init container). Os cont√™ineres de inicializa√ß√£o s√£o executados antes do cont√™iner principal e s√£o projetados para configurar o ambiente necess√°rio para o cont√™iner principal. Eles devem concluir com sucesso antes que os cont√™ineres principais comecem.
        
        Neste caso, o cont√™iner de inicializa√ß√£o `install` est√° usando a imagem `busybox` para executar o comando `wget` e baixar a p√°gina inicial do site Docker. Ele ent√£o salva essa p√°gina como `index.html` no volume `workdir`.
        
        O volume `workdir` √© um volume do tipo `emptyDir`, o que significa que ele √© inicialmente vazio e √© criado quando o Pod √© atribu√≠do a um n√≥. Qualquer dado que √© escrito no volume √© preservado atrav√©s de reinicializa√ß√µes do cont√™iner.
        
        O cont√™iner principal `nginx` tamb√©m monta o volume `workdir` no caminho `/usr/share/nginx/html`, que √© o diret√≥rio onde o nginx serve arquivos est√°ticos por padr√£o. Portanto, quando o cont√™iner `nginx` come√ßa, ele j√° tem o arquivo `index.html` que foi baixado pelo cont√™iner de inicializa√ß√£o, e ele pode servir esse arquivo quando recebe uma solicita√ß√£o HTTP.
        
        Ent√£o, para responder √† sua pergunta, o cont√™iner `install` baixa o site usando `busybox` e `wget` porque `busybox` √© uma imagem leve que inclui o utilit√°rio `wget`, e o site baixado pode ser servido pelo cont√™iner `nginx` porque ambos os cont√™ineres compartilham o volume `workdir`.
        
   ## RBAC
    
    
    RBAC (Role-Based Access Control) √© um m√©todo de **controle de acesso** que √© usado no Kubernetes para gerenciar permiss√µes sobre quem pode fazer o qu√™ dentro do cluster.
    
    No RBAC, voc√™ define pol√≠ticas de acesso em termos de Roles e RoleBindings:
    
    - Uma `Role` define um conjunto de permiss√µes, como a capacidade de criar pods ou listar servi√ßos. Essas permiss√µes s√£o definidas em termos de recursos do Kubernetes (como pods, services, etc.) e opera√ß√µes (como get, list, create, delete, etc.).
    - Uma `RoleBinding` associa uma Role a um ou mais usu√°rios, grupos ou ServiceAccounts. Isso significa que esses usu√°rios, grupos ou ServiceAccounts t√™m as permiss√µes definidas na Role.
    - 
    
    H√° tamb√©m `ClusterRoles` e `ClusterRoleBindings`, que s√£o como Roles e RoleBindings, mas se aplicam a todo o cluster, n√£o apenas a um namespace espec√≠fico.
    
    Aqui est√° um exemplo de uma Role e uma RoleBinding no Kubernetes:
    
    ```yaml
    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      namespace: default
      name: pod-reader
    rules:
    - apiGroups: [""]
      resources: ["pods"]
      verbs: ["get", "watch", "list"]
    ---
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: read-pods
      namespace: default
    subjects:
    - kind: User
      name: jane
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: Role
      name: pod-reader
      apiGroup: rbac.authorization.k8s.io
    
    ```
    
    Neste exemplo, a Role `pod-reader` permite que o usu√°rio leia pods no namespace `default`. A RoleBinding `read-pods` concede essa Role ao usu√°rio `jane`. Portanto, `jane` pode ler pods no namespace `default`.
    
    - **ServiceAccount**: No Kubernetes, um ServiceAccount √© um objeto que identifica processos que rodam em um Pod. Em outras palavras, enquanto usu√°rios humanos usam contas de usu√°rio para acessar recursos do Kubernetes, aplica√ß√µes usam ServiceAccounts. Quando um Pod √© criado, ele √© automaticamente associado a um ServiceAccount. Se nenhum for especificado, o ServiceAccount `default` √© usado. Tokens de ServiceAccount s√£o armazenados como Secrets e montados em Pods.
    
    - **ClusterRole**: Um ClusterRole √© um objeto que define um conjunto de permiss√µes que podem ser aplicadas em todo o cluster, ou em um conjunto espec√≠fico de namespaces, para um conjunto espec√≠fico de recursos (por exemplo, Pods, Services, etc.). As permiss√µes s√£o definidas em termos de opera√ß√µes (como get, list, create, delete, etc.). Por exemplo, um ClusterRole pode permitir a leitura de todos os Pods em todos os namespaces.
    
    - **ClusterRoleBinding**: Um ClusterRoleBinding √© um objeto que associa um ClusterRole a um conjunto de usu√°rios, grupos ou ServiceAccounts. Isso significa que esses usu√°rios, grupos ou ServiceAccounts t√™m as permiss√µes definidas no ClusterRole. Por exemplo, um ClusterRoleBinding pode conceder a um ServiceAccount espec√≠fico a permiss√£o de ler todos os Pods em todos os namespaces.
    
    Aqui est√° um exemplo de como esses tr√™s conceitos podem ser usados juntos:
    
    ```yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: my-service-account
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: pod-reader
    rules:
    - apiGroups: [""]
      resources: ["pods"]
      verbs: ["get", "watch", "list"]
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: read-pods
    subjects:
    - kind: ServiceAccount
      name: my-service-account
      namespace: default
    roleRef:
      kind: ClusterRole
      name: pod-reader
      apiGroup: rbac.authorization.k8s.io
    
    ```
    
    Neste exemplo, um ServiceAccount chamado `my-service-account` √© criado. Um ClusterRole chamado `pod-reader` √© criado, que permite a leitura de Pods. Um ClusterRoleBinding chamado `read-pods` √© criado, que associa o ServiceAccount `my-service-account` ao ClusterRole `pod-reader`. Portanto, qualquer Pod que use o ServiceAccount `my-service-account` pode ler Pods em todos os namespaces.
    
    Criando um service account via linda de comando:
    
    ```yaml
    kubectl create serviceaccount erick
    
    kubectl get serviceaccount
    ```
    
    Listando cluster roles
    
    ```yaml
    kubectl get clusterrole
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2034.png)
    
    Pegando informa√ß√£o de um determinado cluster role
    
    ```yaml
    kubectl describe clusterrole cluster-admin
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2035.png)
    
    `cluster-admin` no Kubernetes. Aqui est√° o que cada parte do resumo significa:
    
    - `Name: cluster-admin`: Este √© o nome do ClusterRole. `cluster-admin` √© um ClusterRole predefinido que d√° permiss√µes completas sobre todos os recursos.
    - `Labels: kubernetes.io/bootstrapping=rbac-defaults`: Labels s√£o pares chave-valor que s√£o anexados aos objetos para fins de organiza√ß√£o e sele√ß√£o. Neste caso, o label indica que este ClusterRole √© parte do bootstrap padr√£o do RBAC do Kubernetes.
    - `Annotations: rbac.authorization.kubernetes.io/autoupdate: true`: As anota√ß√µes s√£o semelhantes aos labels, mas s√£o usadas para anexar informa√ß√µes n√£o identific√°veis aos objetos. Neste caso, a anota√ß√£o indica que este ClusterRole deve ser atualizado automaticamente pelo sistema.
    
    - `PolicyRule:`: Esta se√ß√£o lista as regras de pol√≠tica que definem as permiss√µes concedidas por este ClusterRole.
        - `Resources *.*`: Esta regra indica que o ClusterRole tem permiss√µes sobre todos os recursos em todos os grupos de recursos.
        - `Non-Resource URLs [*]`: Esta regra indica que o ClusterRole tem permiss√µes sobre todas as URLs n√£o associadas a recursos.
        - `Resource Names []`: Como esta lista est√° vazia, a regra n√£o se aplica a nenhum nome de recurso espec√≠fico, mas a todos os recursos.
        - `Verbs [*]`: Esta regra indica que o ClusterRole tem permiss√µes para realizar todas as a√ß√µes (get, list, create, delete, etc.) em todos os recursos e URLs n√£o associadas a recursos.
        
    
    Em resumo, o ClusterRole `cluster-admin` tem permiss√µes completas para fazer qualquer coisa em qualquer recurso ou URL n√£o associada a recursos no cluster.
    
    ```yaml
    kubectl describe clusterrole view
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2036.png)
    
    Listando cluster role biding
    
    ```yaml
    kubectl get clusterrolebindings
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2037.png)
    
    ```yaml
    kubectl describe clusterrolebindings cluster-admin
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2038.png)
    
    Criando um **clusterrolebindings** via linda de comando
    
    ```yaml
    kubectl create clusterrolebinding yodai --serviceaccount=default:erick --clusterrole=cluster-admin
    
    kubectl get clusterrolebinding
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2039.png)
    
    ```yaml
    kubectl describe clusterrolebinding yodai
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2040.png)
    
    ```yaml
    kubectl describe serviceaccount erick
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2041.png)
    
    YAML ServiceAccount
    
    ```yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: admin-user
      namespace: kube-system
    ```
    
    YAML ClusterRoleBinding
    
    ```yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: admin-user
    roleRef:
      # vai dar permiss√£o de administrador para o usu√°rio admin-user
      apiGroup: rbac.authorization.k8s.io/v1
      # Associando o usu√°rio admin-user ao cluster-admin
      kind: ClusterRole
      name: cluster-admin
    # √© poss√≠vel associar um usu√°rio a um grupo de usu√°rios ou a um servi√ßo de conta
    # Associando o service account admin-user ao clustr-admin
    subjects:
    - kind: ServiceAccount
      name: admin-user
      namespace: kube-system
    ```
    
     Este √© um objeto `ClusterRoleBinding` no Kubernetes. Ele est√° associando um `ServiceAccount` chamado `admin-user` no namespace `kube-system` ao `ClusterRole` chamado `cluster-admin`. Aqui est√° o que cada parte do arquivo faz:
    
    - `apiVersion: rbac.authorization.k8s.io/v1`: Define a vers√£o da API do Kubernetes que este objeto pertence.
    - `kind: ClusterRoleBinding`: Define o tipo de objeto Kubernetes que est√° sendo criado. Neste caso, √© um `ClusterRoleBinding`.
    - `metadata:`: Cont√©m metadados sobre o objeto, como seu nome.
        - `name: admin-user`: Define o nome do ClusterRoleBinding.
    - `roleRef:`: Define a refer√™ncia para o ClusterRole que est√° sendo associado.
        - `apiGroup: rbac.authorization.k8s.io/v1`: Define o grupo de API do ClusterRole.
        - `kind: ClusterRole`: Define o tipo de objeto ao qual a refer√™ncia se aplica.
        - `name: cluster-admin`: Define o nome do ClusterRole ao qual a refer√™ncia se aplica.
    - `subjects:`: Define a lista de sujeitos que ser√£o associados ao ClusterRole.
        - `kind: ServiceAccount`: Define o tipo de sujeito que est√° sendo associado.
        - `name: admin-user`: Define o nome do ServiceAccount que est√° sendo associado.
        - `namespace: kube-system`: Define o namespace do ServiceAccount que est√° sendo associado.
    
    Portanto, este ClusterRoleBinding est√° dando ao ServiceAccount `admin-user` no namespace `kube-system` as permiss√µes definidas no ClusterRole `cluster-admin`, que geralmente inclui permiss√µes completas para todos os recursos em todo o cluster.
    
    ```yaml
    kubectl create -f admin-cluster-role-binding.yaml
    kubectl get clusterrolebinding
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2042.png)
    
    ```yaml
    kubectl describe clusterrolebinding admin-user
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2043.png)
    
  ## **Helm**
    
    
    O Helm √© uma ferramenta popular de c√≥digo aberto para gerenciar aplicativos Kubernetes. Kubernetes √© um sistema de orquestra√ß√£o de cont√™ineres amplamente utilizado para automatizar a implanta√ß√£o, escalonamento e gerenciamento de cont√™ineres em ambientes de produ√ß√£o.
    
    O Helm simplifica o processo de empacotamento, distribui√ß√£o e gerenciamento de aplicativos Kubernetes. Ele permite que voc√™ defina, instale e atualize facilmente aplicativos complexos em um cluster Kubernetes. Aqui est√£o alguns dos conceitos-chave associados ao Helm:
    
    1. **Chart:** Um pacote Helm √© chamado de "chart". Um chart √© uma cole√ß√£o de arquivos que descrevem um conjunto de recursos do Kubernetes. Ele inclui defini√ß√µes para implanta√ß√µes, servi√ßos, ingressos, configura√ß√µes e outros recursos necess√°rios para executar um aplicativo em um cluster Kubernetes.
    2. **Release:** Uma inst√¢ncia espec√≠fica de um chart em um cluster Kubernetes √© chamada de "release". Cada release representa uma implanta√ß√£o espec√≠fica de um aplicativo em execu√ß√£o.
    3. **Values:** Os charts Helm podem ser parametrizados usando arquivos de valores. Isso permite que voc√™ personalize a configura√ß√£o do aplicativo, como a quantidade de r√©plicas, portas, senhas, etc., sem modificar o pr√≥prio chart.
    4. **Reposit√≥rios:** Os charts Helm podem ser armazenados em reposit√≥rios para facilitar o compartilhamento e a distribui√ß√£o. Helm permite que voc√™ pesquise e instale charts diretamente de reposit√≥rios remotos.
    5. **Helm CLI:** Helm √© usado principalmente por meio de uma interface de linha de comando (CLI). Com a CLI do Helm, voc√™ pode instalar, atualizar, listar e desinstalar charts em clusters Kubernetes.
    
    - Helm ‚Üí √â uma forma de instalar de determinados softwares, pacotes, sem a necessidade de instalar manualmente.
    - Imagine que o Helm seria um apt-get da vida
    - Existem algumas formas de se instalar o helm
    
    1. Via snap
        
        ```yaml
        snap install helm --classic
        ```
        
    
    1. Via wget
        
        ```bash
        wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.3-linux-amd64.tar.gz
        tar -vxzf helm-v2.11.0-linux-amd64.tar.gz
        
        cd linux-amd64/
        
        mv helm /usr/local/bin/
        
        mv tiller /usr/local/bin/
        
        helm
        ```
        
    
    1. Via brew
        
        ```bash
        brew install helm
        ```
        
    
     Para quem esta utilizando o kubeadm, √© preciso se atentar com rela√ß√£o ao rbac, ou seja,  o helm precisa estar associado ao **clusterrollebinding**
    
    Siga o passo-a-passo:
    
    ```bash
    create serviceaccount --namespace=kube-system tiller
    kubrctl create clusterrolebinding tiller-cluster-role --clusterrole cluster-admin --serviceaccount=kube-system:tiller
    
    kubectl patch deployments -n kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
    ```
    
    **Com o helm instalado, agora instale o prometheus como exemplo:**
    
    ```bash
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm install prometheus prometheus-community/prometheus --version 25.8.0
    ```
    
    Assim que o promethueus for instalado no cluster, o tipo de servi√ßo dele ser√° um clusterIp, v√° at√© o service do prometheus-server e d√™ um edit e edite o tipo de service em type.
    
    ```bash
    kubectl edit service 
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2044.png)
    
    Ap√≥s editar, siga o passo a passo abaixo:
    
    ```bash
    kubectl get service
    
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2045.png)
    
    ```bash
    # pegue o ip do N√ì de controle
    https://ip:31245 -> digite isso na url de seu navegador
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2046.png)
    
    **Agora suba um deployment para que o prometheus pegue as informa√ß√µes de m√©tricas desse container:**
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: giropops-v1
      labels:
        app: giropops
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: giropops
      template:
        metadata:
          labels:
            app: giropops
          # passando informa√ß√£o para o cluster que o pod pode ser monitorado pelo prometheus
          annotations:
            # Vai come√ßar a pegar informa√ß√µes automaticamente do container que est√° rodando na porta 8080
            prometheus.io/scrape: "true"
            # Porta que vai expor as m√©tricas
            prometheus.io/port: "32111"
        spec:
          containers:
          - name: giropops
            image: linuxtips/nginx-prometheus-exporter:1.0.0
            env:
            - name: VERSION
              value: "1.0.0"
            ports:
            - containerPort: 80
            - containerPort: 32111
    ```
    
    **Agora clique em Status > Target, e verifique se encontra o pod que subimos**
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2047.png)
    
    **Entre no pod e d√™ um curl dentro so POD para verificar as m√©tricas do container**
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2048.png)
    
    **V√° na parte inicial do Prometheus e digite: nginx_http_requests**
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2049.png)
    
    **Instalando o Grafana no cluster via Helm**
    
    ```bash
    helm repo add grafana https://grafana.github.io/helm-charts
    helm install my-grafana grafana/grafana --version 7.0.9
    ```
    
    **Usu√°rio e senha padr√£o do grafana**
    
    ```bash
    
    kubectl get secret --namespace monitoring my-grafana -o jsonpath="{.data.admin-user}" | base64 --decode ; echo
    kubectl get secret --namespace monitoring my-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
    ```
    
    Troque o tipo de Service do grafana por NodePort
    
    **Adicionando um data source no Grafana**
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2050.png)
    
    Criando um Dashboard
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2051.png)
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2052.png)
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2053.png)
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2054.png)
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2055.png)
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2056.png)
    
    **Comandos Helm:**
    
    ```yaml
    # Listar os reposit√≥rios
    helm repo list
    helm delete <nome>
    
    helm delete <nome> --purge # Vai deletar de formar completa
    
    # Remover um reposit√≥rio espec√≠fico (substitua "nome-do-repositorio" pelo nome real)
    helm repo remove nome-do-repositorio
    
    # Verificar se o reposit√≥rio foi removido
    helm repo list
    # desinstalar determinado repo
    helm uninstall my-prometheus
    ```
    
  ## **Ingress**
    
    o `Ingress` √© um recurso que gerencia o acesso externo aos servi√ßos dentro de um cluster. Ele fornece um conjunto de regras para rotear o tr√°fego externo para os servi√ßos internos com base em diferentes crit√©rios, como o nome do host, caminho da URL e outros.
    
    Os principais objetivos do `Ingress` incluem:
    
    1. **Roteamento de Tr√°fego:** O `Ingress` permite rotear o tr√°fego externo para servi√ßos espec√≠ficos com base em regras definidas. Isso √© √∫til quando voc√™ tem v√°rios servi√ßos em execu√ß√£o no cluster e deseja direcionar o tr√°fego com base em URLs espec√≠ficas.
    2. **Termina√ß√£o SSL/TLS:** O `Ingress` tamb√©m pode ser configurado para fornecer termina√ß√£o SSL/TLS, permitindo que o tr√°fego externo seja criptografado at√© o `Ingress` antes de ser roteado para os servi√ßos internos.
    3. **Balanceamento de Carga:** Pode ser utilizado em conjunto com controladores de balanceamento de carga para distribuir o tr√°fego de entrada entre os pods dos servi√ßos.
    
    Para usar o `Ingress`, √© necess√°rio ter um controlador de `Ingress` implantado no cluster. Controladores populares incluem o Nginx Ingress Controller e o Traefik. O controlador √© respons√°vel por implementar as regras definidas nos recursos `Ingress` e gerenciar o roteamento do tr√°fego.
    
    - O ingress √© uma maneira de abstrair os services
    - Fazer com que os services sejam expostos para al√©m do cluster
    - Fazer com que os servi√ßos rodem fora do cluster
    
    Aqui est√° um exemplo simples de um recurso `Ingress`:
    
    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: meu-ingress
    spec:
      rules:
      - host: meuservico.minhadomina.com
        http:
          paths:
          - path: /caminho1
            pathType: Prefix
            backend:
              service:
                name: servico1
                port:
                  number: 80
          - path: /caminho2
            pathType: Prefix
            backend:
              service:
                name: servico2
                port:
                  number: 8080
    
    ```
    
    Neste exemplo, o tr√°fego para `meuservico.minhadomina.com/caminho1` ser√° roteado para o servi√ßo chamado `servico1`, enquanto o tr√°fego para `meuservico.minhadomina.com/caminho2` ser√° roteado para o servi√ßo chamado `servico2`.
    
    Comando para pegar v√°rios Objetos
    
    ```bash
    kubectl get deploy,svc,ep,po
    ```
    
    **CRIANDO UM CLUSTER K8S COM NGINX INGRESS CONTROLLER EM SUA M√ÅQUINA**
    
    ```yaml
    kind: Cluster
    apiVersion: kind.x-k8s.io/v1alpha4
    nodes:
    - role: control-plane
      kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "ingress-ready=true"
      extraPortMappings:
      - containerPort: 80
        hostPort: 80
        protocol: TCP
      - containerPort: 443
        hostPort: 443
        protocol: TCP
    ```
    
    - `kind: Cluster`: Indica que este √© um arquivo de configura√ß√£o para criar um cluster Kind.
    - `apiVersion: kind.x-k8s.io/v1alpha4`: Especifica a vers√£o da API Kind a ser usada.
    - `nodes`: Lista os n√≥s do cluster. No caso, h√° um n√≥.
        - `role: control-plane`: Define que este n√≥ ser√° um n√≥ de plano de controle, o que significa que ter√° todos os componentes principais do Kubernetes.
        - `kubeadmConfigPatches`: Permite aplicar patches √† configura√ß√£o do `kubeadm` usado para inicializar o n√≥. No caso, um patch √© aplicado √† se√ß√£o `InitConfiguration` para adicionar argumentos extras ao `kubelet`. Este patch adiciona um r√≥tulo (`ingress-ready=true`) ao n√≥.
        - `extraPortMappings`: Mapeia portas adicionais entre o host e o cont√™iner.
            - `containerPort: 80`: Porta do cont√™iner a ser mapeada.
            - `hostPort: 80`: Porta do host √† qual a porta do cont√™iner ser√° mapeada.
            - `protocol: TCP`: Protocolo a ser usado para o mapeamento de porta.
            - O mesmo padr√£o √© repetido para a porta 443.
    
    Criando um ingress nginx controller
    
    ```yaml
    kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2057.png)
    
    Os manifestos cont√™m patches espec√≠ficos do tipo para encaminhar os hostPorts para o controlador de ingresso, definir toler√¢ncias de contamina√ß√£o e agend√°-los para o n√≥ com r√≥tulo personalizado.
    
    Agora o Ingress est√° todo configurado. Aguarde at√© que esteja pronto para processar as solicita√ß√µes em execu√ß√£o:
    
    ```yaml
    kubectl wait --namespace ingress-nginx \
      --for=condition=ready pod \
      --selector=app.kubernetes.io/component=controller \
      --timeout=90s
    ```
    
    YAML testando o ingress
    
    ```yaml
    kind: Pod
    apiVersion: v1
    metadata:
      name: yoda-app
      labels:
        app: yoda
    spec:
      containers:
      - command:
        - /agnhost
        - netexec
        - --http-port
        - "8080"
        image: registry.k8s.io/e2e-test-images/agnhost:2.39
        name: yoda-app
    ---
    kind: Service
    apiVersion: v1
    metadata:
      name: yoda-service
    spec:
      selector:
        app: yoda
      ports:
      # Default port used by the image
      - port: 8080
    ---
    kind: Pod
    apiVersion: v1
    metadata:
      name: dioday-app
      labels:
        app: dioday
    spec:
      containers:
      - command:
        - /agnhost
        - netexec
        - --http-port
        - "8080"
        image: registry.k8s.io/e2e-test-images/agnhost:2.39
        name: dioday-app
    ---
    kind: Service
    apiVersion: v1
    metadata:
      name: dioday-service
    spec:
      selector:
        app: dioday
      ports:
      # Default port used by the image
      - port: 8080
    ---
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: example-ingress
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /$2
    spec:
      rules:
      - http:
          paths:
          - pathType: Prefix
            path: /yoda
            backend:
              service:
                name: yoda-service
                port:
                  number: 8080
          - pathType: Prefix
            path: /dioday
            backend:
              service:
                name: dioday-service
                port:
                  number: 8080
    ```
    
    Quando tudo estiver rodando, fa√ßa os seguinte testes com curl
    
    ```bash
    curl localhost/yoda
    curl localhost/dioday
    ```
    
    ![Untitled](Kubernetes%20730e37146baa485d9f46957707434f50/Untitled%2058.png)
    
  ## Assuntos-Extra
    
    ## **Comandos Extras**
    
    ```bash
    kubeadm token create --print-join-command
    ```
    
    **Fazer um autocomplete do kubectl**
    
    ```bash
    kubectl completion bash > /etc
    ```
    
    **Mas primeiro, veja se esta instalado o bash-completion, caso nao esteja instalado, instale com o seguinte comando:**
    
    ```bash
    apt-get install -y bash-completion
    ```
    
    **Redirecionar o completion para o etc**
    
    ```bash
    kubectl completion bash > /etc/bash_completion
    ```
    
    **For√ßando ele ler e atualizar a modificacao**
    
    ```bash
    source <(kubectl completion bash)
    ```
    
    ---
    
    **COMANDOS BASICOS**
    
    - Um pod e como se fosse uma jaula onde os container compartilham do mesmo namespace e do mesmo IP do POD
    - namespace -> e um lugar isolado, cercadinho da aplicacao
    
    ```bash
    kubectl get pods
    kubectl get pods -n kube-system
    ```
    
    ```bash
    kubectl describe pods -n kube-system <nomePod>
    kubectl get pods --all-namespace
    kubectl get pods --all-namespace -o wide
    ```
    
    ```bash
    kubectl get namespaces
    ```
    
    ---
    
    **Entrando dentro de determinado Pod e instalando o Vim dentro do container do nginx**
    
    ```bash
    kubectl exec -it meu-nginx-d9cd787bb-6qw92 -- /bin/bash
    
    # ==========================================================
    
    # Dentro do Container
    apt update
    apt install vim
    
    # Va ate o caminho do index.html e altere-o
    root@meu-nginx-d9cd787bb-6qw92:/usr/share/nginx/html# vim index.html
    ```
    
    ## **Conte√∫do complementar - EstudosKubernetes**
    
    [https://github.com/Erick-Fernandes-dev/EstudosKubernetes](https://github.com/Erick-Fernandes-dev/EstudosKubernetes)


## HPA

### Instalando o metrics server

MetricServer ‚Üí Coletar as m√©tricas em tempo real de quanto cada Pod, cada parte do Kubernetes est√° consumindo naquele momento

N√£o v√™m instaldo no kind por padr√£o

Link para instalar o metrics-server

[https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server)

Baixe component: üëáüèª

```go
wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

v√° em issues: üëáüèª

[https://github.com/kubernetes-sigs/metrics-server/issues/525](https://github.com/kubernetes-sigs/metrics-server/issues/525)

![Untitled](Kubernetes%2076b569f94cdc4598a04fae48c3ab5045/Untitled.png)

Manifesto do metrics server

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: system:aggregated-metrics-reader
rules:
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
rules:
- apiGroups:
  - ""
  resources:
  - nodes/metrics
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: metrics-server
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    k8s-app: metrics-server
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    k8s-app: metrics-server
  name: metrics-server
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  strategy:
    rollingUpdate:
      maxUnavailable: 0
  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
        image: registry.k8s.io/metrics-server/metrics-server:v0.6.4
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /livez
            port: https
            scheme: HTTPS
          periodSeconds: 10
        name: metrics-server
        ports:
        - containerPort: 4443
          name: https
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /readyz
            port: https
            scheme: HTTPS
          initialDelaySeconds: 20
          periodSeconds: 10
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /tmp
          name: tmp-dir
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      serviceAccountName: metrics-server
      volumes:
      - emptyDir: {}
        name: tmp-dir
---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  labels:
    k8s-app: metrics-server
  name: v1beta1.metrics.k8s.io
spec:
  group: metrics.k8s.io
  groupPriorityMinimum: 100
  insecureSkipTLSVerify: true
  service:
    name: metrics-server
    namespace: kube-system
  version: v1beta1
  versionPriority: 100
```

Listar os apiservices

```yaml
kubectl get apiservices
```

### Entendendo utiliza√ß√£o de Resources

vCPU ‚Üí 1000m  (milicores) 500m. 0.5 ‚Üí metade da minha vCPU

Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: wesleywillians/hello-go:v9.6
        resources:
          requests:
            memory: "5Mi"
            cpu: "0.1"
          limits:
            memory: "10Mi"
            cpu: "0.2"
        ports:
        - containerPort: 8000
```

Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8000
```

Rodando o Fortio

```yaml
kubectl run --image=fortio/fortio fortio -- load -qps 800 -t 20s -c 50 "http://myapp-service/healthz"
```

Estressando mais ainda

```yaml
kubectl run --image=fortio/fortio fortio -- load -qps 900 -t 20s -c 80 "http://myapp-service/healthz"
```

```yaml
watch -n1 kubectl top pod myapp-599756b6c7-vdh5w
```

### Finalmente o HPA

O HPA (Horizontal Pod Autoscaler) no Kubernetes √© um recurso que automatiza a escala do n√∫mero de r√©plicas de um conjunto de pods (replica set, deployment, stateful set, etc.) com base na carga de trabalho ou m√©tricas espec√≠ficas. O objetivo √© garantir que sua aplica√ß√£o tenha a capacidade computacional necess√°ria para lidar com as demandas vari√°veis.

A principal finalidade do HPA √© permitir que sua aplica√ß√£o se ajuste dinamicamente ao tr√°fego, escalando para cima quando h√° um aumento na carga e para baixo quando a carga diminui. Aqui est√£o alguns pontos-chave sobre o HPA:

1. **Autoscaling com Base em M√©tricas:**
    - O HPA monitora m√©tricas espec√≠ficas, como uso de CPU ou m√©tricas personalizadas, e ajusta o n√∫mero de r√©plicas do conjunto de pods com base nessas m√©tricas.
2. **Configura√ß√£o Flex√≠vel:**
    - Voc√™ pode configurar o HPA para escalar com base em m√©tricas de uso de CPU, uso de mem√≥ria ou at√© mesmo m√©tricas personalizadas definidas por voc√™.
3. **Suporte a M√©tricas Personalizadas:**
    - Al√©m das m√©tricas padr√£o do sistema, o HPA suporta m√©tricas personalizadas, permitindo que voc√™ ajuste a escala com base em aspectos espec√≠ficos de sua aplica√ß√£o.
4. **Integra√ß√£o com M√∫ltiplos Tipos de Workloads:**
    - O HPA pode ser utilizado com diferentes tipos de workloads, incluindo deployments, stateful sets e replica sets.
5. **Grada√ß√£o de Escala:**
    - O HPA permite que voc√™ defina regras para escalonamento gradual, evitando ajustes bruscos e proporcionando uma adapta√ß√£o mais suave √†s mudan√ßas na carga.
6. **Preven√ß√£o contra Oscila√ß√£o:**
    - O HPA inclui mecanismos para evitar oscila√ß√µes frequentes de escalonamento, o que ajuda a estabilizar a opera√ß√£o em situa√ß√µes de carga flutuante.
7. **Integra√ß√£o com o Cluster Autoscaler:**
    - Quando usado em conjunto com o Cluster Autoscaler, o HPA pode escalar n√£o apenas o n√∫mero de pods em um deployment, mas tamb√©m escalar o n√∫mero de n√≥s no cluster, proporcionando uma escalabilidade ainda mais abrangente.

Ao usar o HPA, voc√™ pode garantir que sua aplica√ß√£o tenha recursos suficientes para lidar com varia√ß√µes na carga de trabalho, garantindo efici√™ncia operacional e uma experi√™ncia mais consistente para os usu√°rios finais.

YAML HPA do deployment myapp

```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  # Vai fazer o autoscaling no deployment myapp
  scaleTargetRef:
    apiVersion: apps/v1
    name: myapp
    kind: Deployment
  # vai rodar 1 replica no minimo e 15 no maximo
  minReplicas: 1
  maxReplicas: 30
  # porcentagem de uso de cpu para escalar
  targetCPUUtilizationPercentage: 50
```

```bash
watch -n1 kubectl get hpa
```

O comando `watch` √© uma ferramenta de linha de comando que executa periodicamente um comando espec√≠fico e exibe a sa√≠da em tempo real. No caso do comando que voc√™ forneceu:

```bash
watch -n1 kubectl get hpa

```

- `watch`: Inicia a execu√ß√£o do comando de forma repetida em intervalos regulares.
- `n1`: Define o intervalo de atualiza√ß√£o, indicando que o comando deve ser executado a cada 1 segundo.
- `kubectl get hpa`: √â o comando Kubernetes que exibe informa√ß√µes sobre os Autoscalers Horizontais (Horizontal Pod Autoscalers ou HPAs) no cluster.

Portanto, esse comando espec√≠fico est√° sendo utilizado para monitorar e exibir as informa√ß√µes sobre os Autoscalers Horizontais no seu cluster Kubernetes. A sa√≠da ser√° atualizada a cada segundo, permitindo que voc√™ observe altera√ß√µes din√¢micas nos valores do HPA, como a contagem atual de r√©plicas, m√©tricas de uso de recursos, entre outras informa√ß√µes relacionadas ao dimensionamento autom√°tico. Esse tipo de monitoramento em tempo real pode ser √∫til para entender como o escalonamento autom√°tico est√° respondendo √†s mudan√ßas na carga de trabalho do cluster.

## Statefulset

O Statefulset √© uma funcionalidade no kubernetes que gerencia o deployment e o scaling de um conjunto de Pods, fornecendo garantias sobre a ordem de deployment e a singularidade desses Pods.

√â distinto entre os Deployments e Replicasets que s√£o considerados stateless (sem estado), os `Statefulsets` s√£o utilizados quando voc√™ precisa de mais garantias sobre o deployment e scaling. Eles garantem que os nomes e endere√ßos dos Pods sejam consistentes e est√°veis ao longo do tempo.

Diferentemente de `Deployments`, que s√£o usados para aplicativos sem estado, `StatefulSets` s√£o ideais para aplica√ß√µes que mant√™m um estado, como bancos de dados e sistemas de armazenamento. Cada inst√¢ncia do pod em um `StatefulSet` tem um nome √∫nico e persistente, e elas s√£o criadas e escaladas de maneira sequencial.

### Quando usar StatefulSets?

Os `StatefulSets` s√£o √∫teis para aplica√ß√µes que necessitam de um ou mais dos seguintes:

- Identidade de rede est√°vel e √∫nica.
- Armazenamento persistente est√°vel.
- Ordem de deployment e scaling garantida.
- Ordem de rolling updates e rollbacks garantida.
- Algumas aplica√ß√µes que se encaixam nesses requisitos s√£o bancos de
dados, sistemas de filas e quaisquer aplicativos que necessitam de
persist√™ncia de dados ou identidade de rede est√°vel.

---

- **dentidade persistente:** Cada pod em um `StatefulSet` possui um nome persistente que √© baseado em um padr√£o, geralmente no formato `<nome-do-statefulset>-<ordinal>`. Essa identidade persistente √© mantida mesmo durante reinicializa√ß√µes ou reimplanta√ß√µes.
- **Persist√™ncia de armazenamento:** `StatefulSets` suportam volumes persistentes, o que significa que os dados podem ser mantidos mesmo se um pod for movido para outro n√≥ ou se for reiniciado.
- **Ordem de inicializa√ß√£o garantida:** A inicializa√ß√£o dos pods em um `StatefulSet` √© feita de forma sequencial, garantindo que cada pod anterior esteja em execu√ß√£o e pronto antes que o pr√≥ximo seja iniciado. Isso √© √∫til para aplica√ß√µes que dependem da ordem durante a inicializa√ß√£o.
- **Servi√ßos de cabe√ßalho:** `StatefulSets` s√£o normalmente associados a servi√ßos de cabe√ßalho (headless services), que fornecem uma resolu√ß√£o DNS est√°vel para cada pod, facilitando a descoberta de outros pods no conjunto.
- **Atualiza√ß√µes controladas:** Atualiza√ß√µes em um `StatefulSet` podem ser controladas para garantir a estabilidade da aplica√ß√£o. Isso √© especialmente √∫til para bancos de dados, onde √© necess√°rio garantir a consist√™ncia dos dados durante atualiza√ß√µes.
    
    
    ### E como ele funciona?
    
    Os `StatefulSets` funcionam criando uma s√©rie de Pods 
    replicados. Cada r√©plica √© uma inst√¢ncia da mesma aplica√ß√£o que √© criada
     a partir do mesmo spec, mas pode ser diferenciada por seu √≠ndice e 
    hostname.
    
    Ao contr√°rio dos Deployments e Replicasets, onde as r√©plicas s√£o 
    intercambi√°veis, cada Pod em um StatefulSet tem um √≠ndice persistente e 
    um hostname que se vinculam a sua identidade.
    
    Por exemplo, se um StatefulSet tiver um nome giropops e um spec com 
    tr√™s r√©plicas, ele criar√° tr√™s Pods: giropops-0, giropops-1, giropops-2.
     A ordem dos √≠ndices √© garantida. O Pod giropops-1 n√£o ser√° iniciado at√©
     que o Pod giropops-0 esteja dispon√≠vel e pronto.
    
    A mesma garantia de ordem √© aplicada ao scaling e aos updates.
    
    ### O StatefulSet e os volumes persistentes
    
    Um aspecto chave dos `StatefulSets` √© a integra√ß√£o com 
    Volumes Persistentes. Quando um Pod √© recriado, ele se reconecta ao 
    mesmo Volume Persistente, garantindo a persist√™ncia dos dados entre as 
    recria√ß√µes dos Pods.
    
    Por padr√£o, o Kubernetes cria um PersistentVolume para cada Pod em um
     StatefulSet, que √© ent√£o vinculado a esse Pod para a vida √∫til do 
    StatefulSet.
    
    Isso √© √∫til para aplica√ß√µes que precisam de um armazenamento persistente e est√°vel, como bancos de dados.
    
    ### O StatefulSet e os volumes persistentes
    
    Um aspecto chave dos `StatefulSets` √© a integra√ß√£o com 
    Volumes Persistentes. Quando um Pod √© recriado, ele se reconecta ao 
    mesmo Volume Persistente, garantindo a persist√™ncia dos dados entre as 
    recria√ß√µes dos Pods.
    
    Por padr√£o, o Kubernetes cria um PersistentVolume para cada Pod em um
     StatefulSet, que √© ent√£o vinculado a esse Pod para a vida √∫til do 
    StatefulSet.
    
    Isso √© √∫til para aplica√ß√µes que precisam de um armazenamento persistente e est√°vel, como bancos de dados.
    
    ### O StatefulSet e o Headless Service
    
    Para entender a rela√ß√£o entre o StatefulSet e o Headless Service, √© preciso primeiro entender o que √© um Headless Service.
    
    No Kubernetes, um servi√ßo √© uma abstra√ß√£o que define um conjunto 
    l√≥gico de Pods e uma maneira de acess√°-los. Normalmente, um servi√ßo tem 
    um IP e encaminha o tr√°fego para os Pods. No entanto, um Headless 
    Service √© um tipo especial de servi√ßo que n√£o tem um IP pr√≥prio. Em vez 
    disso, ele retorna diretamente os IPs dos Pods que est√£o associados a 
    ele.
    
    Agora, o que isso tem a ver com os `StatefulSets`?
    
    Os `StatefulSets` e os `Headless Services` geralmente trabalham juntos no gerenciamento de aplica√ß√µes stateful. O `Headless Service` √© respons√°vel por permitir a comunica√ß√£o de rede entre os Pods em um `StatefulSet`, enquanto o ` gerencia o deployment e o scaling desses Pods.
    
    Aqui est√° como eles funcionam juntos:
    
    Quando um `StatefulSet` √© criado, ele geralmente √© associado a um `Headless Service`. Ele √© usado para controlar o dom√≠nio DNS dos `Pods` criados pelo `StatefulSet`. Cada `Pod` obt√©m um nome de host DNS que segue o formato: `<pod-name>.<service-name>.<namespace>.svc.cluster.local`. Isso permite que cada `Pod` seja alcan√ßado individualmente.
    
    Por exemplo, se voc√™ tiver um `StatefulSet` chamado giropops com tr√™s r√©plicas e um `Headless Service` chamado `nginx`, os `Pods`
     criados ser√£o giropops-0, giropops-1, giropops-2 e eles ter√£o os 
    seguintes endere√ßos de host DNS: 
    
    giropops-0.nginx.default.svc.cluster.local, 
    giropops-1.nginx.default.svc.cluster.local, 
    giropops-2.nginx.default.svc.cluster.local.
    
    Essa combina√ß√£o de `StatefulSets` com `Headless Services` permite que aplica√ß√µes `stateful`,
     como bancos de dados, tenham uma identidade de rede est√°vel e 
    previs√≠vel, facilitando a comunica√ß√£o entre diferentes inst√¢ncias da 
    mesma aplica√ß√£o.
    
    ### Criando um StatefulSet:
    
    ```yaml
    apiVersion: apps/v1
    kind: StatefulSet # Tipo do recurso que estamos criando, no caso, um StatefulSet
    metadata:
      name: nginx
    spec:
      serviceName: "nginx"
      replicas: 3
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx
            ports:
            - containerPort: 80
              name: web
            volumeMounts:
            - name: www
              mountPath: /usr/share/nginx/html
      volumeClaimTemplates: # Como estamos utilizando StatefulSet, precisamos criar um template de volume para cada Pod, ent√£ ao inv√©s de criarmos um volume diretamente, criamos um template que ser√° utilizado para criar um volume para cada Pod
      - metadata:
          name: www # Nome do volume, assim teremos o volume www-0, www-1 e www-2
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 1Gi
    ```
    
    ```yaml
    kubectl apply -f nginx-statefulset.yaml
    
    # Ver o statefulset
    kubectl get statefulset
    NAME    READY   AGE
    nginx   3/3     2m38s
    
    # vizualizar mais informa√ß√µes
    kubectl describe statefulset nginx
    
    Name:               nginx
    Namespace:          default
    CreationTimestamp:  Thu, 18 May 2023 23:44:45 +0200
    Selector:           app=nginx
    Labels:             <none>
    Annotations:        <none>
    Replicas:           3 desired | 3 total
    Update Strategy:    RollingUpdate
      Partition:        0
    Pods Status:        3 Running / 0 Waiting / 0 Succeeded / 0 Failed
    Pod Template:
      Labels:  app=nginx
      Containers:
       nginx:
        Image:        nginx
        Port:         80/TCP
        Host Port:    0/TCP
        Environment:  <none>
        Mounts:
          /usr/share/nginx/html from www (rw)
      Volumes:  <none>
    Volume Claims:
      Name:          www
      StorageClass:  
      Labels:        <none>
      Annotations:   <none>
      Capacity:      1Gi
      Access Modes:  [ReadWriteOnce]
    Events:
      Type    Reason            Age   From                    Message
      ----    ------            ----  ----                    -------
      Normal  SuccessfulCreate  112s  statefulset-controller  create Claim www-nginx-0 Pod nginx-0 in StatefulSet nginx success
      Normal  SuccessfulCreate  112s  statefulset-controller  create Pod nginx-0 in StatefulSet nginx successful
      Normal  SuccessfulCreate  102s  statefulset-controller  create Claim www-nginx-1 Pod nginx-1 in StatefulSet nginx success
      Normal  SuccessfulCreate  102s  statefulset-controller  create Pod nginx-1 in StatefulSet nginx successful
      Normal  SuccessfulCreate  96s   statefulset-controller  create Claim www-nginx-2 Pod nginx-2 in StatefulSet nginx success
      Normal  SuccessfulCreate  96s   statefulset-controller  create Pod nginx-2 in StatefulSet nginx successful
    ```
    
    ```yaml
    kubectl get pods
    NAME      READY   STATUS    RESTARTS   AGE
    nginx-0   1/1     Running   0          24s
    nginx-1   1/1     Running   0          14s
    nginx-2   1/1     Running   0          8s
    ```
    
    O nosso `StatefulSet` est√° criado, mas ainda temos que criar o `Headless Service` para que possamos acessar os `Pods` individualmente, e para isso, vamos criar o arquivo `nginx-headless-service.yaml` com o seguinte conte√∫do:
    
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      ports:
      - port: 80
        name: web
      clusterIP: None # Como estamos criando um Headless Service, n√£o queremos que ele tenha um IP, ent√£o definimos o clusterIP como None
      selector:
        app: nginx
    ```
    
    ```yaml
    kubectl apply -f nginx-headless-service.yaml
    
    kubectl get service
    
    kubectl describe service nginx
    
    ```
    
    Agora que j√° temos o `StatefulSet` e o `Headless Service` criados, podemos acessar cada `Pod` individualmente, para isso, vamos utilizar o comando:
    
    ```yaml
    kubectl run -it --rm debug --image=busybox --restart=Never -- sh
    ```
    
    Agora vamos utilizar o comando `nslookup` para verificar o endere√ßo IP de cada `Pod`, para isso, vamos utilizar o comando:
    
    ```yaml
    nslookup nginx-0.nginx
    ```
    
    Agora vamos acessar o `Pod` utilizando o endere√ßo IP, para isso, vamos utilizar o comando:
    
    ```yaml
    wget -O- http://<endere√ßo-ip-do-pod>
    ```
    
    Precisamos mudar a p√°gina web de cada `Pod` para que possamos verificar se estamos acessando o `Pod` correto, para isso, vamos utilizar o comando:
    
    ```
    echo "Pod 0" > /usr/share/nginx/html/index.html
    ```
    
    Agora vamos acessar o `Pod` novamente, para isso, vamos utilizar o comando:
    
    ```
    wget -O- http://<endere√ßo-ip-do-pod>
    ```
    
    A sa√≠da do comando deve ser:
    
    ```
    Connecting to <endere√ßo-ip-do-pod>:80 (<endere√ßo-ip-do-pod>:80)
    Pod 0
    ```
    
    Caso queira, voc√™ pode fazer o mesmo para os outros `Pods`, basta mudar o n√∫mero do `Pod` no comando `nslookup` e no comando `echo`.
    
    ### Excluindo um StatefulSet
    
    Para excluir um `StatefulSet` precisamos utilizar o comando:
    
    ```
    kubectl delete statefulset nginx
    
    ```
    
    Ou ainda podemos excluir o `StatefulSet` utilizando o comando:
    
    ```
    kubectl delete -f nginx-statefulset.yaml
    
    ```
    
    ### Excluindo um Headless Service
    
    Para excluir um `Headless Service` precisamos utilizar o comando:
    
    ```
    kubectl delete service nginx
    
    ```
    
    Ou ainda podemos excluir o `Headless Service` utilizando o comando:
    
    ```
    kubectl delete -f nginx-headless-service.yaml
    
    ```
    
    ### Excluindo um PVC
    
    Para excluir um `Volume` precisamos utilizar o comando:
    
    ```
    kubectl delete pvc www-0
    
    ```
    

## Extra

## Comandos:

Entrar dentro de um Container dentro de um Pod:

```yaml
kubectl attach <nome_pod> -c <nome_container> -ti
```
